{
  "id": 7366224,
  "user_id": 606786,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 328,
  "type": "post",
  "title": "Lecture 25: Test-time compute, sampling, and RL post-training",
  "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/1chm7oisBBtHpGT4MEgh7z8Q\" filename=\"Lecture 25.pdf\"/><paragraph>Use this thread to discuss the lecture. </paragraph><paragraph>I misspoke during lecture at the end when I was trying to explain the min term. This is a reward that is being maximized and so the min here is stopping large values from becoming too large (not the other way around). The paper discussed clipping in both directions (both are done in different papers) but suggests that it is important to stop accidentally having very large weights to a single generation/token due to a mismatch in the ratio. I misspoke in lecture but the expressions written are correct. Sorry.</paragraph><paragraph/><paragraph/></document>",
  "document": "Use this thread to discuss the lecture. \n\nI misspoke during lecture at the end when I was trying to explain the min term. This is a reward that is being maximized and so the min here is stopping large values from becoming too large (not the other way around). The paper discussed clipping in both directions (both are done in different papers) but suggests that it is important to stop accidentally having very large weights to a single generation/token due to a mismatch in the ratio. I misspoke in lecture but the expressions written are correct. Sorry.\n\n\n\n",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 0,
  "view_count": 788,
  "unique_view_count": 185,
  "vote_count": 0,
  "reply_count": 4,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": true,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-11-26T11:36:33.911819+11:00",
  "updated_at": "2025-12-18T10:27:22.59362+11:00",
  "deleted_at": null,
  "pinned_at": "2025-11-26T11:36:33.911452+11:00",
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": true,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": "2025-12-04T12:55:53.929253+11:00",
  "new_reply_count": 4,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 17267915,
      "user_id": 698319,
      "course_id": 84647,
      "thread_id": 7366224,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing my notes for Lecture 25. I covered both the test-time compute approaches (prompting, repeated generation, power sampling with MCMC) and the RLVR/ScaleRL training objective, breaking down each component of the loss function.</paragraph><paragraph>Tried to add context for why each piece of the GRPO objective exists (importance sampling ratios, clipping, stop gradients, etc.) since that was the part I found trickiest to follow live.</paragraph><paragraph>If you catch any errors or want a specific section expanded, let me know and I'll update it.</paragraph><paragraph>I used an LLM to help generate notes using the notes I took while reviewing the lecture slides.</paragraph><paragraph>Hope this helps, please do let me know if you want me to expand on anything!<break/></paragraph><file url=\"https://static.us.edusercontent.com/files/cb2cqoauWwXrcwGF6A8sgGR2\" filename=\"Lecture 25 Notes Post Training Test.pdf\"/></document>",
      "document": "Sharing my notes for Lecture 25. I covered both the test-time compute approaches (prompting, repeated generation, power sampling with MCMC) and the RLVR/ScaleRL training objective, breaking down each component of the loss function.\n\nTried to add context for why each piece of the GRPO objective exists (importance sampling ratios, clipping, stop gradients, etc.) since that was the part I found trickiest to follow live.\n\nIf you catch any errors or want a specific section expanded, let me know and I'll update it.\n\nI used an LLM to help generate notes using the notes I took while reviewing the lecture slides.\n\nHope this helps, please do let me know if you want me to expand on anything!\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-09T11:17:04.004087+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17296957,
      "user_id": 622883,
      "course_id": 84647,
      "thread_id": 7366224,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph><link href=\"https://arxiv.org/pdf/2506.14245\">https://arxiv.org/pdf/2506.14245</link></paragraph><paragraph><break/>This was a very interesting paper I found relating to these topics. It discusses how past Pass@K (whether or not a model can generate a correct response within K attempts) evaluation work has demonstrated that RLVR actually can decrease Pass@K accuracy, as opposed to base models. However, this paper argues that RLVR helps lead to better reasoning regardless, and that by instead using a novel evaluation metric they title \"CoT-Pass@K\" designed to reward responses with correct reasoning along the way as well as a correct answer, the RLVR-trained model achieves a higher accuracy than base. They did this task specifically on AIME math problems which was interesting. One cool thing they found was that the Pass@K and CoT-Pass@K metrics for the RLVR-trained model were not very different on the 2024 AIME, indicating that the model's reasoning was quite sound after RLVR!</paragraph></document>",
      "document": "https://arxiv.org/pdf/2506.14245\n\n\nThis was a very interesting paper I found relating to these topics. It discusses how past Pass@K (whether or not a model can generate a correct response within K attempts) evaluation work has demonstrated that RLVR actually can decrease Pass@K accuracy, as opposed to base models. However, this paper argues that RLVR helps lead to better reasoning regardless, and that by instead using a novel evaluation metric they title \"CoT-Pass@K\" designed to reward responses with correct reasoning along the way as well as a correct answer, the RLVR-trained model achieves a higher accuracy than base. They did this task specifically on AIME math problems which was interesting. One cool thing they found was that the Pass@K and CoT-Pass@K metrics for the RLVR-trained model were not very different on the 2024 AIME, indicating that the model's reasoning was quite sound after RLVR!",
      "flag_count": 0,
      "vote_count": 2,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T13:37:05.989941+11:00",
      "updated_at": "2025-12-11T18:34:12.046938+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17300103,
          "user_id": 957592,
          "course_id": 84647,
          "thread_id": 7366224,
          "original_id": null,
          "parent_id": 17296957,
          "editor_id": null,
          "number": 4,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Thank you for this paper it was an interesting read. A topic that seems worthy of further research is how to design evaluation metrics that can distinguish correct reasoning structure from superficial or shortcut-based reasoning across tasks beyond math, such as coding or scientific problem-solving.</paragraph><paragraph/></document>",
          "document": "Thank you for this paper it was an interesting read. A topic that seems worthy of further research is how to design evaluation metrics that can distinguish correct reasoning structure from superficial or shortcut-based reasoning across tasks beyond math, such as coding or scientific problem-solving.\n\n",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-11T18:35:34.142159+11:00",
          "updated_at": "2025-12-11T18:54:15.7833+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 17297901,
      "user_id": 961865,
      "course_id": 84647,
      "thread_id": 7366224,
      "original_id": null,
      "parent_id": null,
      "editor_id": 961865,
      "number": 3,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph><link href=\"https://arxiv.org/pdf/2506.10947\">https://arxiv.org/pdf/2506.10947</link> <break/><break/>I thought this paper was quite interesting, since it applies this idea of \"spurious rewards\" for RLVR that don't depend on the actual ground truth. For instance, they sometimes added random rewards, incorrect rewards, or format rewards (rewarding responses that contained \\boxed{}). An interesting finding from this paper was that even such spurious rewards yielded large improvements on Math, but these improvements were highly model dependent. They showed that for Qwen2.5-Math models, spurious rewards produced large math gains, but this effect did not generalize to other models. I found it super interesting how even using random rewards or simply incorrect rewards still led to improvements. One hypothesis for why this occurs that the paper proposes was that RLVR might amplify pre-training biases.</paragraph></document>",
      "document": "https://arxiv.org/pdf/2506.10947 \n\nI thought this paper was quite interesting, since it applies this idea of \"spurious rewards\" for RLVR that don't depend on the actual ground truth. For instance, they sometimes added random rewards, incorrect rewards, or format rewards (rewarding responses that contained \\boxed{}). An interesting finding from this paper was that even such spurious rewards yielded large improvements on Math, but these improvements were highly model dependent. They showed that for Qwen2.5-Math models, spurious rewards produced large math gains, but this effect did not generalize to other models. I found it super interesting how even using random rewards or simply incorrect rewards still led to improvements. One hypothesis for why this occurs that the paper proposes was that RLVR might amplify pre-training biases.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T14:51:35.358271+11:00",
      "updated_at": "2025-12-11T15:10:28.781387+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    }
  ]
}