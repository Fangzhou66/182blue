{
  "id": 6937658,
  "user_id": 650420,
  "course_id": 84647,
  "original_id": null,
  "editor_id": 606786,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 58,
  "type": "post",
  "title": "Lecture 3 thread",
  "content": "<document version=\"2.0\"><paragraph>Following up from the question in lecture: I only partially stated the conditions required for the Lyapunov function to converge, sorry about that. They are written out fully here, and also corrected in the attached notes. The proof is fully correct, I just misspoke. </paragraph><paragraph/><paragraph>To show that the expected loss, and therefore $z$, goes to zero, we have to show that:</paragraph><paragraph>1. It is non-negative.</paragraph><paragraph>2. $L(z) = 0$ implies $z = 0$.</paragraph><paragraph>3. Expected loss decreases at every step, and that there is a constant downward pressure, so it cannot converge somewhere other than zero. For this we show:</paragraph><paragraph>$$E[L(z_{t+1})] \\leq C L(z_t)$$, where $0&lt; C &lt;  1$ is a constant.</paragraph><paragraph>Essentially this shows an exponential decay, and the decay will continue as long as $L(z_t) &gt; 0.$Once we show this, we see that the only place the loss can converge to is zero, because converging anywhere else would mean you could still decrease the expected loss, which would be a contradiction.</paragraph><paragraph/><paragraph>Notes for lecture 2 and 3 are below. </paragraph><file url=\"https://static.us.edusercontent.com/files/Er1pb2HvgYsi7GNsVhdWoKkA\" filename=\"Lecture 2 (1).pdf\"/><file url=\"https://static.us.edusercontent.com/files/NnFTZ6v9HiF05ho2UjNXAqh5\" filename=\"Lecture 3.pdf\"/><paragraph/></document>",
  "document": "Following up from the question in lecture: I only partially stated the conditions required for the Lyapunov function to converge, sorry about that. They are written out fully here, and also corrected in the attached notes. The proof is fully correct, I just misspoke. \n\n\n\nTo show that the expected loss, and therefore $z$, goes to zero, we have to show that:\n\n1. It is non-negative.\n\n2. $L(z) = 0$ implies $z = 0$.\n\n3. Expected loss decreases at every step, and that there is a constant downward pressure, so it cannot converge somewhere other than zero. For this we show:\n\n$$E[L(z_{t+1})] \\leq C L(z_t)$$, where $0< C <  1$ is a constant.\n\nEssentially this shows an exponential decay, and the decay will continue as long as $L(z_t) > 0.$Once we show this, we see that the only place the loss can converge to is zero, because converging anywhere else would mean you could still decrease the expected loss, which would be a contradiction.\n\n\n\nNotes for lecture 2 and 3 are below. \n\n",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 1,
  "view_count": 1565,
  "unique_view_count": 257,
  "vote_count": 1,
  "reply_count": 8,
  "unresolved_count": 2,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-09-10T09:10:53.900393+10:00",
  "updated_at": "2025-12-18T10:52:58.060422+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16159278,
      "user_id": 954911,
      "course_id": 84647,
      "thread_id": 6937658,
      "original_id": null,
      "parent_id": null,
      "editor_id": 954911,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Just want to clarify, it was very clear that gradient descent and sgd applied on a typical least squares/ ridge problem move differently in different sing vec dirns depending on the singular values but we generalized that for our formulation of Adam which is also applied to tons of different neural net architectures. Is this one of those things that we apply the intuition and generalize it and it just works? I'm also assuming the same is for early stopping with different optimizers.</paragraph></document>",
      "document": "Just want to clarify, it was very clear that gradient descent and sgd applied on a typical least squares/ ridge problem move differently in different sing vec dirns depending on the singular values but we generalized that for our formulation of Adam which is also applied to tons of different neural net architectures. Is this one of those things that we apply the intuition and generalize it and it just works? I'm also assuming the same is for early stopping with different optimizers.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-10T13:36:16.856974+10:00",
      "updated_at": "2025-09-10T15:07:26.255592+10:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16237510,
          "user_id": 650420,
          "course_id": 84647,
          "thread_id": 6937658,
          "original_id": null,
          "parent_id": 16159278,
          "editor_id": null,
          "number": 7,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>When are are talking about signSGD (the simplified perspective on Adam) we are not thinking of normalizing long each singular vector direction. SignSGD is taking uniform steps along each component of the vector (i.e. each of directions $\\vec{e}_1, \\vec{e}_2, ...$ of the original problem. So you are right that here that signSGD doesn't get at equalizing along each singular vector direction --- but it does a different kind of normalization, which happens to work well.  </paragraph></document>",
          "document": "When are are talking about signSGD (the simplified perspective on Adam) we are not thinking of normalizing long each singular vector direction. SignSGD is taking uniform steps along each component of the vector (i.e. each of directions $\\vec{e}_1, \\vec{e}_2, ...$ of the original problem. So you are right that here that signSGD doesn't get at equalizing along each singular vector direction --- but it does a different kind of normalization, which happens to work well.  ",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-17T09:35:39.900361+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 16194127,
      "user_id": 647818,
      "course_id": 84647,
      "thread_id": 6937658,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>When will lecture 4 be posted?</paragraph></document>",
      "document": "When will lecture 4 be posted?",
      "flag_count": 0,
      "vote_count": 6,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-13T15:52:36.630944+10:00",
      "updated_at": "2025-09-16T14:53:27.333225+10:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16236134,
          "user_id": 650420,
          "course_id": 84647,
          "thread_id": 6937658,
          "original_id": null,
          "parent_id": 16194127,
          "editor_id": null,
          "number": 6,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Just did. </paragraph></document>",
          "document": "Just did. ",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-17T07:36:54.578529+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 16225016,
      "user_id": 1155278,
      "course_id": 84647,
      "thread_id": 6937658,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 4,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>I was wondering if there's a typo in the end of Lecture2 note, I thought it should be </paragraph><math>E[B|\\tilde{w}_t] \\leq \\frac{4}{n} \\eta^2 \\sigma_{\\max}^2 \\rho^2 \\mathcal{L}(\\tilde{w}_t)</math><paragraph>instead of </paragraph><math>E[B|\\tilde{w}_t] \\leq 2 \\eta^2 \\sigma_{\\max}^2 \\rho^2 \\mathcal{L}(\\tilde{w}_t)</math><paragraph>or did i miss something?</paragraph></document>",
      "document": "I was wondering if there's a typo in the end of Lecture2 note, I thought it should be \n\n$$E[B|\\tilde{w}_t] \\leq \\frac{4}{n} \\eta^2 \\sigma_{\\max}^2 \\rho^2 \\mathcal{L}(\\tilde{w}_t)$$\n\ninstead of \n\n$$E[B|\\tilde{w}_t] \\leq 2 \\eta^2 \\sigma_{\\max}^2 \\rho^2 \\mathcal{L}(\\tilde{w}_t)$$\n\nor did i miss something?",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-16T11:13:51.831427+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16236104,
          "user_id": 650420,
          "course_id": 84647,
          "thread_id": 6937658,
          "original_id": null,
          "parent_id": 16225016,
          "editor_id": null,
          "number": 5,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Yes, thanks for catching it!</paragraph></document>",
          "document": "Yes, thanks for catching it!",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-17T07:34:51.467323+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 16364352,
      "user_id": 1751485,
      "course_id": 84647,
      "thread_id": 6937658,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 8,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/gW6jaYfvUh1akuEW7clcwtWq\" width=\"571\" height=\"264.14302812687015\"/></figure><paragraph><break/>Hi. I just wonder why does multiplying the moving-average parameters by an exponential function of t act as a bias correction? As t grows the corrections from those parameters to the vectors m and v become smaller — is that just to let the parameters move quickly toward good values early on? You can see that if β=0.9, or if β1​=0.9 and β2​=0.1, there can still be a bias and the proportional relationship remains unchanged.<break/></paragraph><paragraph><break/></paragraph><paragraph/><paragraph/></document>",
      "document": "\nHi. I just wonder why does multiplying the moving-average parameters by an exponential function of t act as a bias correction? As t grows the corrections from those parameters to the vectors m and v become smaller — is that just to let the parameters move quickly toward good values early on? You can see that if β=0.9, or if β1​=0.9 and β2​=0.1, there can still be a bias and the proportional relationship remains unchanged.\n\n\n\n\n\n\n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-27T08:18:53.165849+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17243232,
          "user_id": 1016381,
          "course_id": 84647,
          "thread_id": 6937658,
          "original_id": null,
          "parent_id": 16364352,
          "editor_id": null,
          "number": 9,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>In real Adam implementation, the \\beta_1 and \\beta_2 are really different, (the default in PyTorch is (0.9, 0.999)). Let the gradient to be g for the first step, if we don't have the correction, m = 0.1g, v=0.001g^2, and the m/sqrt(v) = 0.1g/sqrt(0.001g^2) = 3.16. However, our expectation for that is 1 for stable learning, which should be fixed by the correction. </paragraph><paragraph> Your statement that 'As t grows the corrections from those parameters to the vectors m and v become smaller — is that just to let the parameters move quickly toward good values early on' is partially right. Without correction, the training will be less stable at the beginning, which might be catastrophic to models as it learns to the wrong directions.</paragraph><paragraph>As steps increases, we want the effect of correction to be less or even no more, because at these times, our momentum and velocities become very close to the 'true' momentum and the velocities of the model on the dataset, where adding corrections will only add noises. </paragraph></document>",
          "document": "In real Adam implementation, the \\beta_1 and \\beta_2 are really different, (the default in PyTorch is (0.9, 0.999)). Let the gradient to be g for the first step, if we don't have the correction, m = 0.1g, v=0.001g^2, and the m/sqrt(v) = 0.1g/sqrt(0.001g^2) = 3.16. However, our expectation for that is 1 for stable learning, which should be fixed by the correction. \n\n Your statement that 'As t grows the corrections from those parameters to the vectors m and v become smaller — is that just to let the parameters move quickly toward good values early on' is partially right. Without correction, the training will be less stable at the beginning, which might be catastrophic to models as it learns to the wrong directions.\n\nAs steps increases, we want the effect of correction to be less or even no more, because at these times, our momentum and velocities become very close to the 'true' momentum and the velocities of the model on the dataset, where adding corrections will only add noises. ",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-07T19:10:57.858696+11:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    }
  ]
}