{
  "id": 7180201,
  "user_id": 606786,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 174,
  "type": "post",
  "title": "Lecture 15: Autoencoder and Self-Supervision Part",
  "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/qI8y8TXguhynXBv9IjNhmuS3\" filename=\"Lecture 15Sahai.pdf\"/><paragraph>This thread is to discuss the lecture part on self-supervision. To keep things clear, the second set of writing building towards state-space models will be kept distinct. </paragraph><paragraph/></document>",
  "document": "This thread is to discuss the lecture part on self-supervision. To keep things clear, the second set of writing building towards state-space models will be kept distinct. \n\n",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 1,
  "view_count": 1106,
  "unique_view_count": 200,
  "vote_count": 1,
  "reply_count": 5,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-10-22T17:30:26.938884+11:00",
  "updated_at": "2025-12-18T03:01:06.507493+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": true,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": "2025-10-29T12:44:42.115658+11:00",
  "new_reply_count": 4,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16737436,
      "user_id": 954911,
      "course_id": 84647,
      "thread_id": 7180201,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>When will the second set of lecture notes be posted?</paragraph></document>",
      "document": "When will the second set of lecture notes be posted?",
      "flag_count": 0,
      "vote_count": 4,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-10-24T15:12:22.345737+11:00",
      "updated_at": "2025-10-29T13:23:37.926772+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 16798870,
      "user_id": 1759011,
      "course_id": 84647,
      "thread_id": 7180201,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>I wonder if Prof.Ranade's slides on SSM can be posted which can greatly help me review and recap. Thanks a lot! </paragraph><paragraph/></document>",
      "document": "I wonder if Prof.Ranade's slides on SSM can be posted which can greatly help me review and recap. Thanks a lot! \n\n",
      "flag_count": 0,
      "vote_count": 3,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-10-29T15:08:07.050355+11:00",
      "updated_at": "2025-10-31T09:13:14.589662+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17152287,
      "user_id": 508873,
      "course_id": 84647,
      "thread_id": 7180201,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 3,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing my notes for Lectures 15. I focused on adding intuition and the \"why\" behind the concepts, closely follows the lecture. Also, have attached a marked up version of Professor Sahai's original lecture notes, adding key points/connections I found most important during the lecture.</paragraph><paragraph>I am happy to share the LaTeX source. Also, if you catch any errors/ want a specific section expanded, just let me know and I'll update it!</paragraph><paragraph>Note: I used an LLM to create the initial structure, but then manually refined and corrected the content through multiple iterations while watching the lectures.</paragraph><file url=\"https://static.us.edusercontent.com/files/RAxsgh9NlCoDRjIwyf0TZl5f\" filename=\"Notes_Lec_15.pdf\"/><paragraph/><file/><file/><file/></document>",
      "document": "Sharing my notes for Lectures 15. I focused on adding intuition and the \"why\" behind the concepts, closely follows the lecture. Also, have attached a marked up version of Professor Sahai's original lecture notes, adding key points/connections I found most important during the lecture.\n\nI am happy to share the LaTeX source. Also, if you catch any errors/ want a specific section expanded, just let me know and I'll update it!\n\nNote: I used an LLM to create the initial structure, but then manually refined and corrected the content through multiple iterations while watching the lectures.\n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-01T18:40:00.548594+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17232085,
      "user_id": 686735,
      "course_id": 84647,
      "thread_id": 7180201,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 4,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>On the topic of self-supervision and learning structure from data with little to no bias, there's Feynman idea of “what I cannot create, I do not understand.” Any thoughts on whether reconstruction (as in MAE) is necessary, sufficient or both for <italic>intelligence</italic> and/or understanding?</paragraph></document>",
      "document": "On the topic of self-supervision and learning structure from data with little to no bias, there's Feynman idea of “what I cannot create, I do not understand.” Any thoughts on whether reconstruction (as in MAE) is necessary, sufficient or both for intelligence and/or understanding?",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-06T17:14:42.505439+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17243516,
          "user_id": 612982,
          "course_id": 84647,
          "thread_id": 7180201,
          "original_id": null,
          "parent_id": 17232085,
          "editor_id": null,
          "number": 5,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I would lean toward: MAE-style reconstruction is neither necessary nor sufficient for “intelligence” or “understanding”.</paragraph><paragraph><bold>1. Why reconstruction (as in MAE) is not sufficient</bold></paragraph><paragraph>In MAE/autoencoders, the model gets a corrupted input $\\tilde{x}$ (eg masked patches) and is trained to <bold>reconstruct</bold> the original $x$ (or the masked parts) in the <italic>same space</italic> (pixels/patches):</paragraph><paragraph>This makes the representation information-rich in the sense that it has to carry enough detail to recover low-level structure. That’s a kind of statistical sufficiency <italic>for the input</italic>, but it doesn’t tell you how the information is organized:</paragraph><list style=\"unordered\"><list-item><paragraph>The model can minimize reconstruction loss by encoding lots of local, pixel detail (textures, patch statistics, exact token identities) without cleanly factorizing object identity, causal structure, or higher-level semantics.</paragraph></list-item><list-item><paragraph>So “I can recreate the pixels” really means “I preserved enough information to invert the corruption,” not “I discovered the right latent variables for understanding the world.”</paragraph></list-item><list-item><paragraph>In the extreme, if you consider the input between two identity layers, you get perfect reconstruction but clearly no interesting representation or abstraction is happening.</paragraph></list-item></list><paragraph>In that sense, reconstruction tends to push toward “sufficient but not minimal”: it keeps a lot of information, not necessarily in the abstract form we care about.</paragraph><paragraph><bold>2. Why reconstruction is not necessary (predictive/ contrastive)</bold></paragraph><paragraph>On the other side, there’s a big family of predictive/contrastive objectives that never try to reconstruct the current input:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Next-token prediction</bold> in language models: given a prefix, predict the next token, but you’re not asked to recreate the entire original sequence from a corrupted version.</paragraph></list-item><list-item><paragraph><bold>JEPA-style</bold> methods: given one view of a scene (the “context”), predict the representation of another view of the same scene, in the latent space rather than input (pixel) space.</paragraph></list-item><list-item><paragraph><bold>Contrastive methods</bold> like SimCLR / DINO: pull together embeddings of two views of the same image and push apart embeddings of different images.</paragraph></list-item></list><paragraph>Intuitively, these objectives encourage the model to keep whatever information is needed to predict the right target view/next token/positive example, and to discard variations that don’t matter for those predictions.</paragraph><paragraph>Empirically, these predictive / contrastive approaches can learn very “understanding-like” representations: objects, dynamics, semantics, etc. That’s strong evidence that explicit reconstruction of $x$ is not necessary for intelligence or understanding.</paragraph><paragraph><bold>3. Two opposite directions toward the same goal</bold></paragraph><paragraph>You can think of all these methods as aiming at the same ideal object: a representation that’s an (approximately) <bold>minimal sufficient statistic</bold> for the underlying world state.</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Reconstruction</bold> comes from the <bold>“don’t throw anything away”</bold> direction: keep enough information to recreate the input, then rely on the bottleneck and architecture to compress along meaningful axes.</paragraph></list-item><list-item><paragraph><bold>Predictive / contrastive</bold> come from the <bold>“only keep what you need to predict”</bold> direction: design views/futures so they share underlying causes, and make the representation sufficient to predict those, discarding view-specific noise.</paragraph></list-item></list><paragraph>Both directions have biases:</paragraph><list style=\"unordered\"><list-item><paragraph>Reconstruction is biased toward low-level fidelity</paragraph></list-item><list-item><paragraph>Predictive/contrastive methods are biased by your choice of views/augmentations and what you ask the model to predict</paragraph></list-item></list></document>",
          "document": "I would lean toward: MAE-style reconstruction is neither necessary nor sufficient for “intelligence” or “understanding”.\n\n1. Why reconstruction (as in MAE) is not sufficient\n\nIn MAE/autoencoders, the model gets a corrupted input $\\tilde{x}$ (eg masked patches) and is trained to reconstruct the original $x$ (or the masked parts) in the same space (pixels/patches):\n\nThis makes the representation information-rich in the sense that it has to carry enough detail to recover low-level structure. That’s a kind of statistical sufficiency for the input, but it doesn’t tell you how the information is organized:\n\nThe model can minimize reconstruction loss by encoding lots of local, pixel detail (textures, patch statistics, exact token identities) without cleanly factorizing object identity, causal structure, or higher-level semantics.\n\nSo “I can recreate the pixels” really means “I preserved enough information to invert the corruption,” not “I discovered the right latent variables for understanding the world.”\n\nIn the extreme, if you consider the input between two identity layers, you get perfect reconstruction but clearly no interesting representation or abstraction is happening.\n\nIn that sense, reconstruction tends to push toward “sufficient but not minimal”: it keeps a lot of information, not necessarily in the abstract form we care about.\n\n2. Why reconstruction is not necessary (predictive/ contrastive)\n\nOn the other side, there’s a big family of predictive/contrastive objectives that never try to reconstruct the current input:\n\nNext-token prediction in language models: given a prefix, predict the next token, but you’re not asked to recreate the entire original sequence from a corrupted version.\n\nJEPA-style methods: given one view of a scene (the “context”), predict the representation of another view of the same scene, in the latent space rather than input (pixel) space.\n\nContrastive methods like SimCLR / DINO: pull together embeddings of two views of the same image and push apart embeddings of different images.\n\nIntuitively, these objectives encourage the model to keep whatever information is needed to predict the right target view/next token/positive example, and to discard variations that don’t matter for those predictions.\n\nEmpirically, these predictive / contrastive approaches can learn very “understanding-like” representations: objects, dynamics, semantics, etc. That’s strong evidence that explicit reconstruction of $x$ is not necessary for intelligence or understanding.\n\n3. Two opposite directions toward the same goal\n\nYou can think of all these methods as aiming at the same ideal object: a representation that’s an (approximately) minimal sufficient statistic for the underlying world state.\n\nReconstruction comes from the “don’t throw anything away” direction: keep enough information to recreate the input, then rely on the bottleneck and architecture to compress along meaningful axes.\n\nPredictive / contrastive come from the “only keep what you need to predict” direction: design views/futures so they share underlying causes, and make the representation sufficient to predict those, discarding view-specific noise.\n\nBoth directions have biases:\n\nReconstruction is biased toward low-level fidelity\n\nPredictive/contrastive methods are biased by your choice of views/augmentations and what you ask the model to predict",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-07T22:01:58.495094+11:00",
          "updated_at": "2025-12-08T09:00:12.338412+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    }
  ]
}