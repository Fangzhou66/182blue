{
  "id": 7007009,
  "user_id": 650420,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 90,
  "type": "post",
  "title": "Lecture 6 and 7 slides and questions",
  "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/5e2fumbMGggxcOkOZ6SBPyQH\" filename=\"Lecture 7.pdf\"/><file url=\"https://static.us.edusercontent.com/files/fJefvrPiaf5ZRgf4YSjzGNTJ\" filename=\"Lecture 6.pdf\"/><paragraph>Muon/MuP questions here!</paragraph></document>",
  "document": "Muon/MuP questions here!",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 1,
  "view_count": 909,
  "unique_view_count": 204,
  "vote_count": 0,
  "reply_count": 12,
  "unresolved_count": 6,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": false,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-09-24T07:31:47.091163+10:00",
  "updated_at": "2025-12-18T10:54:16.485184+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16319385,
      "user_id": 622612,
      "course_id": 84647,
      "thread_id": 7007009,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>How is \"muon key idea 1\" of multiplying by root (dout/din) integrated into the algorithm described on slide 10 of lecture 7?  </paragraph></document>",
      "document": "How is \"muon key idea 1\" of multiplying by root (dout/din) integrated into the algorithm described on slide 10 of lecture 7?  ",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-24T07:47:56.94688+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16320608,
          "user_id": 650420,
          "course_id": 84647,
          "thread_id": 7007009,
          "original_id": null,
          "parent_id": 16319385,
          "editor_id": null,
          "number": 2,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Good question. I folded the factor into the learning rate there but it would be better to be explicit about it. I will post an updated note. </paragraph></document>",
          "document": "Good question. I folded the factor into the learning rate there but it would be better to be explicit about it. I will post an updated note. ",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-24T09:08:09.941629+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 16360534,
      "user_id": 954911,
      "course_id": 84647,
      "thread_id": 7007009,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 3,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/reQ1xdqdpDp8DL0kQas9ku7t\" width=\"655\" height=\"217.96822742474916\"/></figure><paragraph/></document>",
      "document": "",
      "flag_count": 0,
      "vote_count": 3,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-27T03:21:06.736918+10:00",
      "updated_at": "2025-10-05T21:15:27.273267+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 16370527,
      "user_id": 1751485,
      "course_id": 84647,
      "thread_id": 7007009,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 4,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/PEXkoG2UGttD7Rnt4HUCkWyO\" width=\"514\" height=\"108.5111111111111\"/></figure><paragraph>If we meet with these two conditions, what will we get?</paragraph></document>",
      "document": "If we meet with these two conditions, what will we get?",
      "flag_count": 0,
      "vote_count": 1,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-28T02:17:35.234926+10:00",
      "updated_at": "2025-09-28T09:55:10.803267+10:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16370531,
          "user_id": 1751485,
          "course_id": 84647,
          "thread_id": 7007009,
          "original_id": null,
          "parent_id": 16370527,
          "editor_id": null,
          "number": 5,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>It's Lecture 6</paragraph><paragraph/></document>",
          "document": "It's Lecture 6\n\n",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-28T02:18:02.772479+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16371986,
              "user_id": 1751485,
              "course_id": 84647,
              "thread_id": 7007009,
              "original_id": null,
              "parent_id": 16370531,
              "editor_id": null,
              "number": 6,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/VtIZRUIDc5t9qK06RLK4rLOj\" width=\"571\" height=\"428.6804020100502\"/></figure><paragraph>And what's the difference between Shampoo with/without accumulation? How will accumulation influences w？Thank you!</paragraph></document>",
              "document": "\n\nAnd what's the difference between Shampoo with/without accumulation? How will accumulation influences w？Thank you!",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-09-28T04:58:26.307468+10:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        },
        {
          "id": 16374660,
          "user_id": 1759011,
          "course_id": 84647,
          "thread_id": 7007009,
          "original_id": null,
          "parent_id": 16370527,
          "editor_id": null,
          "number": 8,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I guess if so, then in every layer, the RMS norm of input and output $h_l$ will be have a constant bound. This shares the same architectural philosophy with Xavier Initialization. The norm stability of each layers' input and output is helpful for exploiting non-linearity and hyperparameter transfer.</paragraph><paragraph/></document>",
          "document": "I guess if so, then in every layer, the RMS norm of input and output $h_l$ will be have a constant bound. This shares the same architectural philosophy with Xavier Initialization. The norm stability of each layers' input and output is helpful for exploiting non-linearity and hyperparameter transfer.\n\n",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-28T08:57:49.001779+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16375240,
              "user_id": 1751485,
              "course_id": 84647,
              "thread_id": 7007009,
              "original_id": null,
              "parent_id": 16374660,
              "editor_id": null,
              "number": 9,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>Thank you!</paragraph></document>",
              "document": "Thank you!",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-09-28T09:51:44.094817+10:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        }
      ]
    },
    {
      "id": 16374028,
      "user_id": 622831,
      "course_id": 84647,
      "thread_id": 7007009,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 7,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/mz8YyPKJoQRH8DfavlDeUvY8\" width=\"640\" height=\"93.13929313929314\"/></figure><paragraph>I understand how Muon constrains the RMS-&gt;RMS norm of ∆W to be Θ(1), but how do we meet condition (1) of constraining the RMS-&gt;RMS norm of W to be Θ(1)?</paragraph><paragraph>e.g. how do we ensure the weights themselves, not just the updates to the weights, have a bounded RMS norm?</paragraph></document>",
      "document": "I understand how Muon constrains the RMS->RMS norm of ∆W to be Θ(1), but how do we meet condition (1) of constraining the RMS->RMS norm of W to be Θ(1)?\n\ne.g. how do we ensure the weights themselves, not just the updates to the weights, have a bounded RMS norm?",
      "flag_count": 0,
      "vote_count": 1,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-28T08:01:08.706688+10:00",
      "updated_at": "2025-09-28T09:04:31.503103+10:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17300128,
          "user_id": 1751537,
          "course_id": 84647,
          "thread_id": 7007009,
          "original_id": null,
          "parent_id": 16374028,
          "editor_id": null,
          "number": 13,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Condition(1) is established via proper initialization rather than the update rule itself. If weights are initialized such that their spectral norm scales with sqrt(d_out/d_in), the resulting RMS -&gt; RMS norm naturally equates to Theta(1). The Muon optimizer then preserves this state by strictly constraining the updates DeltaW to also be Theta(1) under the same norm, ensuring the weights do not diverge from this bounded scaling regime during training</paragraph></document>",
          "document": "Condition(1) is established via proper initialization rather than the update rule itself. If weights are initialized such that their spectral norm scales with sqrt(d_out/d_in), the resulting RMS -> RMS norm naturally equates to Theta(1). The Muon optimizer then preserves this state by strictly constraining the updates DeltaW to also be Theta(1) under the same norm, ensuring the weights do not diverge from this bounded scaling regime during training",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-11T18:39:12.359207+11:00",
          "updated_at": "2025-12-11T18:40:02.674407+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 16401983,
      "user_id": 1770931,
      "course_id": 84647,
      "thread_id": 7007009,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 11,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>I implemented MuOn on Colab and a simple example with comparison with Adam. Where should I post it for others to see it? Github?</paragraph></document>",
      "document": "I implemented MuOn on Colab and a simple example with comparison with Adam. Where should I post it for others to see it? Github?",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-30T09:38:06.266502+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17299724,
      "user_id": 639518,
      "course_id": 84647,
      "thread_id": 7007009,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 12,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing my notes for Lectures 5, 6, and 7</paragraph><paragraph>Note: I combined my own class notes with the lecture slides and used an LLM to help generate the final summary.</paragraph><paragraph>Hope this is helpful!</paragraph><file url=\"https://static.us.edusercontent.com/files/bE8ZldcpfnRI1YG9gBKGbiCB\" filename=\"lec5-7_chatgpt_lec_summary.pdf\"/></document>",
      "document": "Sharing my notes for Lectures 5, 6, and 7\n\nNote: I combined my own class notes with the lecture slides and used an LLM to help generate the final summary.\n\nHope this is helpful!",
      "flag_count": 0,
      "vote_count": 2,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T17:26:30.958205+11:00",
      "updated_at": "2025-12-16T12:50:44.363833+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    }
  ]
}