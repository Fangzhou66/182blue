{
  "id": 7351756,
  "user_id": 606786,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 307,
  "type": "post",
  "title": "Lecture 24: VAEs and starting test-time compute and post-training",
  "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/yF94T6o6P3n9frsyn6cLcPlz\" filename=\"Lecture 24.pdf\"/><paragraph>This is so students can ask questions about the lecture. I forgot to mention in lecture that Chapters 14 and 17 in Prince are relevant.</paragraph><paragraph>Remember, after starting Generative models, we are going to do a little interlude to go through more details of test-time compute and post-training for LLMs. The reason we're doing this is two-fold: </paragraph><paragraph>A) The VAE discussion, along with the homework, has unlocked some of the technical tools that we are going to need. Since we have enough now, we can return to post-training since it continues our earlier theme of fine-tuning.</paragraph><paragraph>B) Some of the things that we are going to be hitting during post-training will then help us in understanding diffusion-based generative models --- which is what we will do afterwards.</paragraph></document>",
  "document": "This is so students can ask questions about the lecture. I forgot to mention in lecture that Chapters 14 and 17 in Prince are relevant.\n\nRemember, after starting Generative models, we are going to do a little interlude to go through more details of test-time compute and post-training for LLMs. The reason we're doing this is two-fold: \n\nA) The VAE discussion, along with the homework, has unlocked some of the technical tools that we are going to need. Since we have enough now, we can return to post-training since it continues our earlier theme of fine-tuning.\n\nB) Some of the things that we are going to be hitting during post-training will then help us in understanding diffusion-based generative models --- which is what we will do afterwards.",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 2,
  "view_count": 769,
  "unique_view_count": 187,
  "vote_count": 0,
  "reply_count": 2,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-11-23T05:39:42.344447+11:00",
  "updated_at": "2025-12-18T06:37:35.690955+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": true,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": "2025-12-06T10:51:56.687138+11:00",
  "new_reply_count": 2,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 17299767,
      "user_id": 1751537,
      "course_id": 84647,
      "thread_id": 7351756,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing notes for Lec24. Hoping it helps.</paragraph><file url=\"https://static.us.edusercontent.com/files/0MNxNs5rY9Vx5IhG4aaUR3qZ\" filename=\"Notes_Lec24.pdf\"/><paragraph/></document>",
      "document": "Sharing notes for Lec24. Hoping it helps.\n\n",
      "flag_count": 0,
      "vote_count": 1,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T17:35:11.220705+11:00",
      "updated_at": "2025-12-12T23:58:04.440253+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17312403,
      "user_id": 612982,
      "course_id": 84647,
      "thread_id": 7351756,
      "original_id": null,
      "parent_id": null,
      "editor_id": 612982,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/ZVrEZFoTdQa6f6ukOt6hAklv\" width=\"640\" height=\"332.23181257706534\"/></figure><paragraph>One intuition that helped me is the “big bubble / small bubbles” view of the VAE regularizer.</paragraph><list style=\"ordered\"><list-item><paragraph>Start from the usual ELBO (prior $p(z)=\\mathcal N(0,I)$): $$ \\mathrm{ELBO}(x) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - \\mathrm{KL}(q_\\phi(z\\mid x)|p(z)). $$</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Expand the KL and define entropy: $$ \\mathrm{KL}(q_\\phi(z\\mid x)|p(z)) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log q_\\phi(z\\mid x)-\\log p(z)]. $$ $$ H(q_\\phi(z\\mid x))=-\\mathbb E_{q_\\phi(z\\mid x)}[\\log q_\\phi(z\\mid x)]. $$ So: $$ -\\mathrm{KL}(q_\\phi(z\\mid x)|p(z)) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p(z)] + H(q_\\phi(z\\mid x)). $$</paragraph></list-item></list><list style=\"ordered\"><list-item><paragraph>Plug back into the ELBO: $$ \\mathrm{ELBO}(x) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] + \\mathbb E_{q_\\phi(z\\mid x)}[\\log p(z)] + H(q_\\phi(z\\mid x)). $$</paragraph></list-item></list><paragraph>Bubble intuition:</paragraph><list style=\"unordered\"><list-item><paragraph>The KL term in the original ELBO is the “big bubble” constraint: it encourages each posterior $q_\\phi(z\\mid x)$ to stay compatible with the Gaussian prior $p(z)$ (i.e., put mass in high-density regions so sampling $z\\sim p(z)$ works).</paragraph></list-item><list-item><paragraph>The entropy term $H(q_\\phi(z\\mid x))$ in the re-expressed ELBO explains the “thickness” of each small bubble: it discourages $q_\\phi(z\\mid x)$ from collapsing to a delta / single point.</paragraph></list-item></list><paragraph>Together, this encourages a latent space that is both sampleable (prior-shaped) and non-collapsed (thick/smooth).</paragraph><paragraph>Figure credit: Angjoo Kanazawa (<link href=\"https://cs280-berkeley.github.io/lectures/lect13.pdf\">link</link>)</paragraph></document>",
      "document": "One intuition that helped me is the “big bubble / small bubbles” view of the VAE regularizer.\n\nStart from the usual ELBO (prior $p(z)=\\mathcal N(0,I)$): $$ \\mathrm{ELBO}(x) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - \\mathrm{KL}(q_\\phi(z\\mid x)|p(z)). $$\n\nExpand the KL and define entropy: $$ \\mathrm{KL}(q_\\phi(z\\mid x)|p(z)) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log q_\\phi(z\\mid x)-\\log p(z)]. $$ $$ H(q_\\phi(z\\mid x))=-\\mathbb E_{q_\\phi(z\\mid x)}[\\log q_\\phi(z\\mid x)]. $$ So: $$ -\\mathrm{KL}(q_\\phi(z\\mid x)|p(z)) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p(z)] + H(q_\\phi(z\\mid x)). $$\n\nPlug back into the ELBO: $$ \\mathrm{ELBO}(x) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] + \\mathbb E_{q_\\phi(z\\mid x)}[\\log p(z)] + H(q_\\phi(z\\mid x)). $$\n\nBubble intuition:\n\nThe KL term in the original ELBO is the “big bubble” constraint: it encourages each posterior $q_\\phi(z\\mid x)$ to stay compatible with the Gaussian prior $p(z)$ (i.e., put mass in high-density regions so sampling $z\\sim p(z)$ works).\n\nThe entropy term $H(q_\\phi(z\\mid x))$ in the re-expressed ELBO explains the “thickness” of each small bubble: it discourages $q_\\phi(z\\mid x)$ from collapsing to a delta / single point.\n\nTogether, this encourages a latent space that is both sampleable (prior-shaped) and non-collapsed (thick/smooth).\n\nFigure credit: Angjoo Kanazawa (link)",
      "flag_count": 0,
      "vote_count": 1,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-13T00:27:21.713822+11:00",
      "updated_at": "2025-12-17T11:52:42.806345+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    }
  ]
}