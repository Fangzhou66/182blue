{
  "id": 7262818,
  "user_id": 650420,
  "course_id": 84647,
  "original_id": null,
  "editor_id": 606786,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 237,
  "type": "post",
  "title": "Lectures: 16-20 State-space models, Transformer networks, Modern Architectures",
  "content": "<document version=\"2.0\"><paragraph>Here are the lecture notes, sorry for the delay!</paragraph><file url=\"https://static.us.edusercontent.com/files/vITc5qG2PULCIwIF95ScYECz\" filename=\"Lecture 20.pdf\"/><file url=\"https://static.us.edusercontent.com/files/wjbKvQIaJBIqHt2GmWZzg2H6\" filename=\"Lecture 19.pdf\"/><file url=\"https://static.us.edusercontent.com/files/1W3yC9Hb0yfuWEPhBdCPszps\" filename=\"Lecture 18.pdf\"/><file url=\"https://static.us.edusercontent.com/files/kZK6auktgCrh8DP8s6xETsTB\" filename=\"Lecture 17.pdf\"/><file url=\"https://static.us.edusercontent.com/files/4Vs92j7DfrkgDUSnNG4if920\" filename=\"Lecture 16.pdf\"/></document>",
  "document": "Here are the lecture notes, sorry for the delay!",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 2,
  "view_count": 1066,
  "unique_view_count": 205,
  "vote_count": 6,
  "reply_count": 13,
  "unresolved_count": 6,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-11-06T08:54:41.615649+11:00",
  "updated_at": "2025-12-18T09:56:41.986225+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16937949,
      "user_id": 954911,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>When we make A have complex eigenvalues, since it's acting directly on our last timestep's hidden state, what's taking it back to a real number?</paragraph></document>",
      "document": "When we make A have complex eigenvalues, since it's acting directly on our last timestep's hidden state, what's taking it back to a real number?",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-11-10T11:48:02.874385+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16941684,
          "user_id": 686735,
          "course_id": 84647,
          "thread_id": 7262818,
          "original_id": null,
          "parent_id": 16937949,
          "editor_id": 686735,
          "number": 2,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I think the system doesn't go <italic>complex</italic>. Complex eigenvalues imply that the dynamics involve rotation or oscillation in a <italic>real</italic> 2D plane corresponding to each conjugate pair. The real matrix representation ensures the hidden state stays real at every timestep.</paragraph></document>",
          "document": "I think the system doesn't go complex. Complex eigenvalues imply that the dynamics involve rotation or oscillation in a real 2D plane corresponding to each conjugate pair. The real matrix representation ensures the hidden state stays real at every timestep.",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-11-10T17:43:40.218705+11:00",
          "updated_at": "2025-11-11T07:50:35.01705+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16945582,
              "user_id": 606786,
              "course_id": 84647,
              "thread_id": 7262818,
              "original_id": null,
              "parent_id": 16941684,
              "editor_id": null,
              "number": 3,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>Exactly. We make the state complex in spirit by using a 2x2 block for each complex eigenvalue. This gives conjugate pairs. The trick is to recognize that the identity behaves like the real number 1 since it's square is itself. While $\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}$ behaves like the square-root of -1 since squaring it gives negative identity. </paragraph></document>",
              "document": "Exactly. We make the state complex in spirit by using a 2x2 block for each complex eigenvalue. This gives conjugate pairs. The trick is to recognize that the identity behaves like the real number 1 since it's square is itself. While $\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$ behaves like the square-root of -1 since squaring it gives negative identity. ",
              "flag_count": 0,
              "vote_count": 1,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-11-11T04:24:05.482512+11:00",
              "updated_at": "2025-11-11T07:50:26.392198+11:00",
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": [
                {
                  "id": 16948796,
                  "user_id": 954911,
                  "course_id": 84647,
                  "thread_id": 7262818,
                  "original_id": null,
                  "parent_id": 16945582,
                  "editor_id": null,
                  "number": 4,
                  "type": "comment",
                  "kind": "normal",
                  "content": "<document version=\"2.0\"><paragraph>Ahh makes sense, thanks!</paragraph></document>",
                  "document": "Ahh makes sense, thanks!",
                  "flag_count": 0,
                  "vote_count": 0,
                  "is_endorsed": false,
                  "is_anonymous": false,
                  "is_private": false,
                  "is_resolved": false,
                  "created_by_bot_id": null,
                  "created_at": "2025-11-11T07:50:33.429339+11:00",
                  "updated_at": null,
                  "deleted_at": null,
                  "anonymous_id": 0,
                  "vote": 0,
                  "comments": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": 17129121,
      "user_id": 639518,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 5,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Hi i have a question about BERT. In the lec notes it says the sentence adjacency (next sentence) prediction is “predict this on the [SEP] token.” I thought in the original BERT setup, the next sentence prediction (in the secondary loss) is applied to the [cls], and there isn’t a separate loss applied to [sep]. I am not sure if i misunderstood something here.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NhefOoU4bAqSZFNj8c9oi3LH\" width=\"313\" height=\"277\"/></figure></document>",
      "document": "Hi i have a question about BERT. In the lec notes it says the sentence adjacency (next sentence) prediction is “predict this on the [SEP] token.” I thought in the original BERT setup, the next sentence prediction (in the secondary loss) is applied to the [cls], and there isn’t a separate loss applied to [sep]. I am not sure if i misunderstood something here.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-11-28T07:26:15.901719+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17186098,
          "user_id": 620476,
          "course_id": 84647,
          "thread_id": 7262818,
          "original_id": null,
          "parent_id": 17129121,
          "editor_id": null,
          "number": 9,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>In the original BERT implementation, the next sentence prediction loss is applied to the representation of the [CLS] token, not the [SEP] token. The input is <code>[CLS] A [SEP] B [SEP]</code>, and the final hidden state of [CLS] is fed into a 2-way classifier (IsNext / NotNext). There is no separate loss on [SEP]; [SEP] only serves as a sentence boundary marker. So I think the lecture saying “predict this on the [SEP] token” is likely a shorthand or slight inaccuracy rather than a description of the actual implementation.</paragraph></document>",
          "document": "In the original BERT implementation, the next sentence prediction loss is applied to the representation of the [CLS] token, not the [SEP] token. The input is [CLS] A [SEP] B [SEP], and the final hidden state of [CLS] is fed into a 2-way classifier (IsNext / NotNext). There is no separate loss on [SEP]; [SEP] only serves as a sentence boundary marker. So I think the lecture saying “predict this on the [SEP] token” is likely a shorthand or slight inaccuracy rather than a description of the actual implementation.",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-03T17:56:56.98556+11:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 17129379,
      "user_id": 639518,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 6,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Another question about multi-head selft-attention (lec 18, recording 10/30/2025 ~28:00). During lecture there was a question about how we go from the D × N input to D/H × N per head. When i tried to look it up afterwards, I might be mixing things up, but I thought each head usually does a learned linear projection from D -&gt; D/H. But in lecture I wasn’t sure if we were instead talking about just splitting the D channels into H chunks (no projection). Just want to confirm about this. Thanks! Btw, this is the equation i think was in the prince book that is associated with fig 12.6</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/C8F8vxB7i5gb1glhvP981Tr6\" width=\"341\" height=\"78\"/></figure></document>",
      "document": "Another question about multi-head selft-attention (lec 18, recording 10/30/2025 ~28:00). During lecture there was a question about how we go from the D × N input to D/H × N per head. When i tried to look it up afterwards, I might be mixing things up, but I thought each head usually does a learned linear projection from D -> D/H. But in lecture I wasn’t sure if we were instead talking about just splitting the D channels into H chunks (no projection). Just want to confirm about this. Thanks! Btw, this is the equation i think was in the prince book that is associated with fig 12.6",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-11-28T08:38:23.278706+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17187255,
          "user_id": 612982,
          "course_id": 84647,
          "thread_id": 7262818,
          "original_id": null,
          "parent_id": 17129379,
          "editor_id": 612982,
          "number": 10,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Hope this helps (here I am using the convention that each row is a token and each column is a feature, and I have dropped the biases for clarity)</paragraph><list style=\"ordered\"><list-item><paragraph>Start with input $X \\in \\mathbb{R}^{N \\times D}$ (sequence length $N$, model dim $D$).</paragraph></list-item><list-item><paragraph>First apply learned linear projections to get queries, keys, and values: $$Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V,$$ where $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$.</paragraph></list-item><list-item><paragraph>Then we split into heads by reshaping the last dimension: $$Q \\in \\mathbb{R}^{N \\times D} \\rightarrow Q \\in \\mathbb{R}^{N \\times H \\times (D/H)},$$ and similarly for $K$ and $V$. Often we then transpose to $\\mathbb{R}^{H \\times N \\times (D/H)}$ so heads are the first dimension.</paragraph></list-item><list-item><paragraph>For each head $h$, we do scaled dot-product attention: $$\\text{Attn}_h = \\mathrm{softmax}\\left(\\frac{Q_h K_h^\\top}{\\sqrt{D/H}}\\right) V_h.$$</paragraph></list-item><list-item><paragraph>After this, we can think of the attention outputs as having shape $\\mathbb{R}^{H \\times N \\times (D/H)}$. We typically transpose back to $\\mathbb{R}^{N \\times H \\times (D/H)}$ so that we can concatenate the heads along the feature dimension.</paragraph></list-item><list-item><paragraph>We then concatenate the $H$ heads back together along that last dimension to get something in $\\mathbb{R}^{N \\times D}$, and finally apply a learned output projection $W_{\\text{out}}$: $$\\text{output} = \\mathrm{Concat}(\\text{Attn}_1, \\dots, \\text{Attn}_<italic>H) W_</italic>{\\text{out}}.$$</paragraph></list-item></list><paragraph>So the splitting into $H$ heads happens after the learned Q/K/V projections. The splitting and transpose operations themselves (where you mentioned going from $D$ to $D/H$ per head) are just reshapes/re-orderings; they’re not additional learned projections.</paragraph><paragraph>Note that the concatenation of the heads is done along the feature dimension. The matrix $W_{\\text{out}}$ is there to mix those feature dimensions (ie aggregate information across heads).</paragraph></document>",
          "document": "Hope this helps (here I am using the convention that each row is a token and each column is a feature, and I have dropped the biases for clarity)\n\nStart with input $X \\in \\mathbb{R}^{N \\times D}$ (sequence length $N$, model dim $D$).\n\nFirst apply learned linear projections to get queries, keys, and values: $$Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V,$$ where $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$.\n\nThen we split into heads by reshaping the last dimension: $$Q \\in \\mathbb{R}^{N \\times D} \\rightarrow Q \\in \\mathbb{R}^{N \\times H \\times (D/H)},$$ and similarly for $K$ and $V$. Often we then transpose to $\\mathbb{R}^{H \\times N \\times (D/H)}$ so heads are the first dimension.\n\nFor each head $h$, we do scaled dot-product attention: $$\\text{Attn}_h = \\mathrm{softmax}\\left(\\frac{Q_h K_h^\\top}{\\sqrt{D/H}}\\right) V_h.$$\n\nAfter this, we can think of the attention outputs as having shape $\\mathbb{R}^{H \\times N \\times (D/H)}$. We typically transpose back to $\\mathbb{R}^{N \\times H \\times (D/H)}$ so that we can concatenate the heads along the feature dimension.\n\nWe then concatenate the $H$ heads back together along that last dimension to get something in $\\mathbb{R}^{N \\times D}$, and finally apply a learned output projection $W_{\\text{out}}$: $$\\text{output} = \\mathrm{Concat}(\\text{Attn}_1, \\dots, \\text{Attn}_H) W_{\\text{out}}.$$\n\nSo the splitting into $H$ heads happens after the learned Q/K/V projections. The splitting and transpose operations themselves (where you mentioned going from $D$ to $D/H$ per head) are just reshapes/re-orderings; they’re not additional learned projections.\n\nNote that the concatenation of the heads is done along the feature dimension. The matrix $W_{\\text{out}}$ is there to mix those feature dimensions (ie aggregate information across heads).",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-03T23:52:40.816508+11:00",
          "updated_at": "2025-12-03T23:59:30.514664+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 17187261,
              "user_id": 612982,
              "course_id": 84647,
              "thread_id": 7262818,
              "original_id": null,
              "parent_id": 17187255,
              "editor_id": null,
              "number": 11,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/wX6XuMKgPRNwscub7DT7Ikld\" width=\"590\" height=\"378.11843361986627\"/></figure><paragraph>Saw this diagram online too, may be helpful (<link href=\"https://dev-discuss.pytorch.org/t/understanding-multi-head-attention-for-ml-framework-developers/1792\">link</link>)</paragraph></document>",
              "document": "Saw this diagram online too, may be helpful (link)",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-12-03T23:54:43.730975+11:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        }
      ]
    },
    {
      "id": 17152221,
      "user_id": 508873,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 7,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing my notes for Lectures 18,19 &amp; 20. I focused on adding intuition and the \"why\" behind the concepts, closely follows the lecture.</paragraph><paragraph>I am happy to share the LaTeX source. Also, if you catch any errors/ want a specific section expanded, just let me know and I'll update it!</paragraph><paragraph>Note: I used an LLM to create the initial structure, but then manually refined and corrected the content through multiple iterations while watching the lectures.</paragraph><paragraph>Hope this helps everyone save some time!</paragraph><paragraph/><file url=\"https://static.us.edusercontent.com/files/bGu5BRIU65upunYkbczxu9yA\" filename=\"Notes_Lec_18_19_20.pdf\"/><paragraph/></document>",
      "document": "Sharing my notes for Lectures 18,19 & 20. I focused on adding intuition and the \"why\" behind the concepts, closely follows the lecture.\n\nI am happy to share the LaTeX source. Also, if you catch any errors/ want a specific section expanded, just let me know and I'll update it!\n\nNote: I used an LLM to create the initial structure, but then manually refined and corrected the content through multiple iterations while watching the lectures.\n\nHope this helps everyone save some time!\n\n\n\n",
      "flag_count": 0,
      "vote_count": 2,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-12-01T18:29:55.577626+11:00",
      "updated_at": "2025-12-11T17:07:30.257054+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17185580,
      "user_id": 610129,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 8,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>For anyone reviewing, this helped my friend out. These are notes that I took (unfortunately handwritten). It starts off with an introduction of stuff from lecture, then goes into everything covered in Chapter 12 of the Prince textbook, and then goes to whatever more modern stuff was covered in lecture that wasn't covered in the textbook. </paragraph><file url=\"https://static.us.edusercontent.com/files/nmksUfC2WHqfy5hZECeWFFwp\" filename=\"Notes 4.pdf\"/></document>",
      "document": "For anyone reviewing, this helped my friend out. These are notes that I took (unfortunately handwritten). It starts off with an introduction of stuff from lecture, then goes into everything covered in Chapter 12 of the Prince textbook, and then goes to whatever more modern stuff was covered in lecture that wasn't covered in the textbook. ",
      "flag_count": 0,
      "vote_count": 2,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-12-03T16:53:03.930435+11:00",
      "updated_at": "2025-12-11T14:22:16.544254+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17299951,
      "user_id": 957592,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 12,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>https://openreview.net/pdf?id=1b7whO4SfY<break/><break/>I came across this paper in NeurIPS, and it's one of the top papers even though it implements such a small tweak to the transformer architecture. It implements a very small change to the attention head that improves performance and helps prevent attention sink (when the model puts a lot of attention on the first token, a phenomenon observed in almost all models). The papers basically adds a sigmoid gate to the Attention head output, and that small tweak has a lot of impact. Though it was relevant to our discussion of transformers in this class.</paragraph><paragraph/></document>",
      "document": "https://openreview.net/pdf?id=1b7whO4SfY\n\nI came across this paper in NeurIPS, and it's one of the top papers even though it implements such a small tweak to the transformer architecture. It implements a very small change to the attention head that improves performance and helps prevent attention sink (when the model puts a lot of attention on the first token, a phenomenon observed in almost all models). The papers basically adds a sigmoid gate to the Attention head output, and that small tweak has a lot of impact. Though it was relevant to our discussion of transformers in this class.\n\n",
      "flag_count": 0,
      "vote_count": 1,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T18:00:30.896702+11:00",
      "updated_at": "2025-12-12T15:06:29.133503+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17299993,
      "user_id": 1751537,
      "course_id": 84647,
      "thread_id": 7262818,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 13,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing notes for Lec16,17, focusing on SSM. Hoping it helps.</paragraph><file url=\"https://static.us.edusercontent.com/files/zrRzQS3lqeNyghDgSFAEDldS\" filename=\"Notes_Lec16_17.pdf\"/></document>",
      "document": "Sharing notes for Lec16,17, focusing on SSM. Hoping it helps.",
      "flag_count": 0,
      "vote_count": 4,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T18:10:35.772079+11:00",
      "updated_at": "2025-12-17T09:45:52.438799+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    }
  ]
}