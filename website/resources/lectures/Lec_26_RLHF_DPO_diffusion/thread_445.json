{
  "id": 7405023,
  "user_id": 606786,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 445,
  "type": "post",
  "title": "Lecture 26 Thread",
  "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/Ykyum7otxP4qvEJ6jX3DokzX\" filename=\"Lecture 26.pdf\"/><paragraph>This lecture hit RLHF, DPO, and then started diffusion. We'll continue with diffusion in the next lecture. </paragraph></document>",
  "document": "This lecture hit RLHF, DPO, and then started diffusion. We'll continue with diffusion in the next lecture. ",
  "category": "Admin",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 1,
  "view_count": 868,
  "unique_view_count": 169,
  "vote_count": 0,
  "reply_count": 6,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": true,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-12-04T16:48:02.895965+11:00",
  "updated_at": "2025-12-18T10:17:59.222664+11:00",
  "deleted_at": null,
  "pinned_at": "2025-12-04T16:48:02.895509+11:00",
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 17202578,
      "user_id": 921495,
      "course_id": 84647,
      "thread_id": 7405023,
      "original_id": null,
      "parent_id": null,
      "editor_id": 921495,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Related: there is an interesting blog post written a researcher at DeepMind, titled <link href=\"https://sander.ai/2024/09/02/spectral-autoregression.html\">Diffusion is Spectral Autoregression</link>, which shows that diffusion models (for image generation) actually approximate autoregressive generation - in the frequency domain. This is interesting because it demonstrates a real, empirical relationship between diffusion and autoregression.</paragraph></document>",
      "document": "Related: there is an interesting blog post written a researcher at DeepMind, titled Diffusion is Spectral Autoregression, which shows that diffusion models (for image generation) actually approximate autoregressive generation - in the frequency domain. This is interesting because it demonstrates a real, empirical relationship between diffusion and autoregression.",
      "flag_count": 0,
      "vote_count": 3,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-04T17:11:17.98965+11:00",
      "updated_at": "2025-12-08T11:51:32.060284+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17215140,
          "user_id": 686735,
          "course_id": 84647,
          "thread_id": 7405023,
          "original_id": null,
          "parent_id": 17202578,
          "editor_id": null,
          "number": 2,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Here to spark debate. I think that's a very cool and inspiring observation. However, it seems to me like such behavior isn't emergent, but forced by the canonical way of training diffusion models, so not very insightful. The forward diffusion process is autoregressive and we are learning the reverse of it, so isn't as cool as the idea that diffusion learnt and converged into an autoregressive solution, which could be very revealing. Check out the article <link href=\"https://www.fabianfalck.com/posts/spectralauto/\"><italic>Diffusion is not necessarily Spectral Autoregression</italic></link>. On another thought, unlike autoregressive models, diffusion models can't roll out indefinitely, it’s fundamentally a finite, parallel procedure, not an open-ended serial one.</paragraph></document>",
          "document": "Here to spark debate. I think that's a very cool and inspiring observation. However, it seems to me like such behavior isn't emergent, but forced by the canonical way of training diffusion models, so not very insightful. The forward diffusion process is autoregressive and we are learning the reverse of it, so isn't as cool as the idea that diffusion learnt and converged into an autoregressive solution, which could be very revealing. Check out the article Diffusion is not necessarily Spectral Autoregression. On another thought, unlike autoregressive models, diffusion models can't roll out indefinitely, it’s fundamentally a finite, parallel procedure, not an open-ended serial one.",
          "flag_count": 0,
          "vote_count": 2,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-05T13:48:31.578215+11:00",
          "updated_at": "2025-12-13T18:54:33.237069+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 17243322,
      "user_id": 622883,
      "course_id": 84647,
      "thread_id": 7405023,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 3,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Just a fun fact, I know PPO (another very common state-of-the-art algorithm for post-training LMs) wasn't mentioned in this lecture but I recently learned the exact mathematical connection between DPO and PPO (sorry for slight errors in symbols and notation). It can be proven that the optimal PPO policy is such that </paragraph><math>\\left(\\pi_{\\theta}^{\\ast}\\left(y\\left|x\\right|\\right)\\ \\propto\\pi_{ref}\\left(y\\right|x\\right)\\exp\\left(\\frac{1}{\\beta}r_{\\varphi}\\left(x,\\ y\\right)\\right)</math><paragraph>, where the left-hand expression is the probability of choosing an output given an input for the optimal policy, first part of the right-hand expression is the same probability for a reference policy, and second part is the reward for taking that output exponentiated. Taking the log of both sides gives us </paragraph><math>\\left(\\log\\pi_{\\theta}^{\\ast}\\left(y\\left|x\\right|\\right)\\ =\\log\\pi_{ref}\\left(y\\right|x\\right)\\ +\\ \\frac{1}{\\beta}r_{\\varphi}\\left(x,\\ y\\right)\\ +\\ C</math><paragraph>Which when rearranged is</paragraph><math>\\left(B\\left(\\log\\pi_{\\theta}^{\\ast}\\left(y\\left|x\\right|\\right)\\ -\\ \\log\\pi_{ref}\\left(y\\right|x\\right)\\right)\\ \\ +\\ C\\ =\\ r_{\\varphi}\\left(x,\\ y\\right)</math><paragraph>Thus reward model outputs are proportional to log ratio between optimal and reference policy. DPO uses this exact relationship as implicit reward!</paragraph></document>",
      "document": "Just a fun fact, I know PPO (another very common state-of-the-art algorithm for post-training LMs) wasn't mentioned in this lecture but I recently learned the exact mathematical connection between DPO and PPO (sorry for slight errors in symbols and notation). It can be proven that the optimal PPO policy is such that \n\n$$\\left(\\pi_{\\theta}^{\\ast}\\left(y\\left|x\\right|\\right)\\ \\propto\\pi_{ref}\\left(y\\right|x\\right)\\exp\\left(\\frac{1}{\\beta}r_{\\varphi}\\left(x,\\ y\\right)\\right)$$\n\n, where the left-hand expression is the probability of choosing an output given an input for the optimal policy, first part of the right-hand expression is the same probability for a reference policy, and second part is the reward for taking that output exponentiated. Taking the log of both sides gives us \n\n$$\\left(\\log\\pi_{\\theta}^{\\ast}\\left(y\\left|x\\right|\\right)\\ =\\log\\pi_{ref}\\left(y\\right|x\\right)\\ +\\ \\frac{1}{\\beta}r_{\\varphi}\\left(x,\\ y\\right)\\ +\\ C$$\n\nWhich when rearranged is\n\n$$\\left(B\\left(\\log\\pi_{\\theta}^{\\ast}\\left(y\\left|x\\right|\\right)\\ -\\ \\log\\pi_{ref}\\left(y\\right|x\\right)\\right)\\ \\ +\\ C\\ =\\ r_{\\varphi}\\left(x,\\ y\\right)$$\n\nThus reward model outputs are proportional to log ratio between optimal and reference policy. DPO uses this exact relationship as implicit reward!",
      "flag_count": 0,
      "vote_count": 4,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-07T19:44:29.539715+11:00",
      "updated_at": "2025-12-08T15:59:26.30647+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17255177,
          "user_id": 961865,
          "course_id": 84647,
          "thread_id": 7405023,
          "original_id": null,
          "parent_id": 17243322,
          "editor_id": null,
          "number": 4,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Yeah I thought that was quite insightful! This derivation also highlights that enforcing the KL-regularized log-ratio essentially pushes the policy towards the distribution that best explains preferences while being minimally different from the reference policy. That also explains why DPO ends up being quite stable, since its search space is constrained to some KL neighborhood around the reference policy instead of drifting freely during training.</paragraph></document>",
          "document": "Yeah I thought that was quite insightful! This derivation also highlights that enforcing the KL-regularized log-ratio essentially pushes the policy towards the distribution that best explains preferences while being minimally different from the reference policy. That also explains why DPO ends up being quite stable, since its search space is constrained to some KL neighborhood around the reference policy instead of drifting freely during training.",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-08T15:59:20.163583+11:00",
          "updated_at": "2025-12-11T13:45:27.433823+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    },
    {
      "id": 17297574,
      "user_id": 961865,
      "course_id": 84647,
      "thread_id": 7405023,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 5,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph><link href=\"https://www.datocms-assets.com/64837/1763662397-1763646865-olmo_3_technical_report-1.pdf\">This</link> paper is quite interesting. It is the technical report for Olmo 3, which was a recently released fully open-souce model. Their post training recipe includes SFT, DPO and RLVR! </paragraph><paragraph/></document>",
      "document": "This paper is quite interesting. It is the technical report for Olmo 3, which was a recently released fully open-souce model. Their post training recipe includes SFT, DPO and RLVR! \n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T14:25:26.25964+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 17299757,
      "user_id": 1751537,
      "course_id": 84647,
      "thread_id": 7405023,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 6,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Sharing notes for Lecture 26,27, focusing on diffusion models. Hoping it helps.</paragraph><file url=\"https://static.us.edusercontent.com/files/LLBjOdsL87CqLOGCM5o6IDa7\" filename=\"Notes_Lec26_27.pdf\"/></document>",
      "document": "Sharing notes for Lecture 26,27, focusing on diffusion models. Hoping it helps.",
      "flag_count": 0,
      "vote_count": 2,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-11T17:33:18.730525+11:00",
      "updated_at": "2025-12-17T15:41:59.708379+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    }
  ]
}