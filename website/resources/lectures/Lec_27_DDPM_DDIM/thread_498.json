{
  "id": 7412341,
  "user_id": 606786,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 498,
  "type": "post",
  "title": "Lecture 27",
  "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/WphKDgwZ6aCiqLeisXGUGlwq\" filename=\"Lecture 27.pdf\"/><paragraph>This thread is to discuss the final lecture in the course. We finished our treatment of diffusion-model training, DDPM style stochastic sampling using what was trained, and DDIM style deterministic sampling using the same exact network that was trained in diffusion style. This should give you what you need to be able to understand diffusion-style models on your own as well as do the final homework 13.</paragraph><paragraph/></document>",
  "document": "This thread is to discuss the final lecture in the course. We finished our treatment of diffusion-model training, DDPM style stochastic sampling using what was trained, and DDIM style deterministic sampling using the same exact network that was trained in diffusion style. This should give you what you need to be able to understand diffusion-style models on your own as well as do the final homework 13.\n\n",
  "category": "Lectures",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 2,
  "view_count": 1777,
  "unique_view_count": 203,
  "vote_count": 0,
  "reply_count": 9,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": true,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-12-05T16:45:19.25015+11:00",
  "updated_at": "2025-12-18T10:15:01.973126+11:00",
  "deleted_at": null,
  "pinned_at": "2025-12-05T16:45:19.249675+11:00",
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": true,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": "2025-12-06T09:54:37.023911+11:00",
  "new_reply_count": 6,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 17222924,
      "user_id": 612982,
      "course_id": 84647,
      "thread_id": 7412341,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Diffusion models are trained only to match the training distribution, yet they appear to generate <italic>novel</italic> samples rather than just repeating images they have seen. In what sense (if at all) does this go beyond memorization or interpolation/nearest neighbors into something we should actually call <italic>creativity?</italic></paragraph></document>",
      "document": "Diffusion models are trained only to match the training distribution, yet they appear to generate novel samples rather than just repeating images they have seen. In what sense (if at all) does this go beyond memorization or interpolation/nearest neighbors into something we should actually call creativity?",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-12-06T05:21:14.694914+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17223365,
          "user_id": 686735,
          "course_id": 84647,
          "thread_id": 7412341,
          "original_id": null,
          "parent_id": 17222924,
          "editor_id": 686735,
          "number": 2,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I think there are multiple reasons up for debate. Here's one based on a very cool paper I was introduced to earlier in the semester! They come up with a set of heuristics capable of predicting the output of diffusion models.</paragraph><paragraph>For a fixed finite dataset, diffusion has a <italic>theoretical</italic> closed-form target, an ideal score field that lets you know, at every noisy point and every time, how to flow back toward the training distribution. Then, if a model matched such field exactly, then running the reverse ODE would provably land you only on training images again, resulting in perfect memorization of the empirical data. </paragraph><paragraph>However, that closed form lives in huge image space and depends on all training points and all times, so you never represent it directly. Real diffusion models just approximate it with a CNN (for instance), and it's exactly their inability to hit this ideal field that leaves room for generalization and creativity.</paragraph><paragraph>As <italic><link href=\"https://arxiv.org/abs/2412.20292\">An analytic theory of creativity in convolutional diffusion models</link></italic> shows, the architectural inductive biases of locality and translational equivariance force models to operate patch-wise rather than globally, enabling combinatorial recombination of training patches into novel but locally valid configurations. This yield a locally consistent patch mosaic mechanism of creativity where exponentially many new images are formed by mixing compatible fragments from disparate training examples.</paragraph><paragraph>So I guess diffusion model creativity isn't magical extrapolation, but structured generalization emerging from imperfect score-learning under inductive biases. The model recombines strongly learned local statistics into new global arrangements.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/MYzk23TOcIa93zrZMf4GTv1j\" width=\"609\" height=\"391.36263157894734\"/></figure><paragraph>Very rough notes I took while I was going through the paper.</paragraph></document>",
          "document": "I think there are multiple reasons up for debate. Here's one based on a very cool paper I was introduced to earlier in the semester! They come up with a set of heuristics capable of predicting the output of diffusion models.\n\nFor a fixed finite dataset, diffusion has a theoretical closed-form target, an ideal score field that lets you know, at every noisy point and every time, how to flow back toward the training distribution. Then, if a model matched such field exactly, then running the reverse ODE would provably land you only on training images again, resulting in perfect memorization of the empirical data. \n\nHowever, that closed form lives in huge image space and depends on all training points and all times, so you never represent it directly. Real diffusion models just approximate it with a CNN (for instance), and it's exactly their inability to hit this ideal field that leaves room for generalization and creativity.\n\nAs An analytic theory of creativity in convolutional diffusion models shows, the architectural inductive biases of locality and translational equivariance force models to operate patch-wise rather than globally, enabling combinatorial recombination of training patches into novel but locally valid configurations. This yield a locally consistent patch mosaic mechanism of creativity where exponentially many new images are formed by mixing compatible fragments from disparate training examples.\n\nSo I guess diffusion model creativity isn't magical extrapolation, but structured generalization emerging from imperfect score-learning under inductive biases. The model recombines strongly learned local statistics into new global arrangements.\n\nVery rough notes I took while I was going through the paper.",
          "flag_count": 0,
          "vote_count": 2,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-06T05:53:49.313328+11:00",
          "updated_at": "2025-12-11T10:04:23.329221+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 17227180,
              "user_id": 612982,
              "course_id": 84647,
              "thread_id": 7412341,
              "original_id": null,
              "parent_id": 17223365,
              "editor_id": null,
              "number": 3,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>That is a very cool paper - and thanks for your explanation and notes! Wondering how this generalizes to DITs? </paragraph></document>",
              "document": "That is a very cool paper - and thanks for your explanation and notes! Wondering how this generalizes to DITs? ",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-12-06T09:51:09.747463+11:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": [
                {
                  "id": 17228216,
                  "user_id": 915218,
                  "course_id": 84647,
                  "thread_id": 7412341,
                  "original_id": null,
                  "parent_id": 17227180,
                  "editor_id": null,
                  "number": 5,
                  "type": "comment",
                  "kind": "normal",
                  "content": "<document version=\"2.0\"><paragraph>I think since DiTs can theoretically memorize global structures better than CNNs with self attention, their generalization likely comes less from \"spatial limitations\" and more from the combinatorial nature of the embedding space. In other words, DiT's creativity comes from semantic composition?</paragraph><paragraph/></document>",
                  "document": "I think since DiTs can theoretically memorize global structures better than CNNs with self attention, their generalization likely comes less from \"spatial limitations\" and more from the combinatorial nature of the embedding space. In other words, DiT's creativity comes from semantic composition?\n\n",
                  "flag_count": 0,
                  "vote_count": 1,
                  "is_endorsed": false,
                  "is_anonymous": false,
                  "is_private": false,
                  "is_resolved": false,
                  "created_by_bot_id": null,
                  "created_at": "2025-12-06T10:59:59.748913+11:00",
                  "updated_at": "2025-12-06T14:27:00.886143+11:00",
                  "deleted_at": null,
                  "anonymous_id": 0,
                  "vote": 0,
                  "comments": []
                }
              ]
            }
          ]
        },
        {
          "id": 17227607,
          "user_id": 957592,
          "course_id": 84647,
          "thread_id": 7412341,
          "original_id": null,
          "parent_id": 17222924,
          "editor_id": null,
          "number": 4,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>The way I think about it is that diffusion models aren’t memorizing images at all more like they’re learning the patterns that make something look like the kind of data they were trained on since during training every image gets completely scrambled with noise, and the model’s only job is to learn how to slowly turn that noise back into something realistic. And since it starts from pure noise when generating, there’s no stored picture it could “pull out,” so it has to build a new one by following those learned patterns. That’s why the outputs aren’t just copies or nearest neighbors, they’re new images that fit the style of the dataset without being any specific example from it. So in that sense, it’s not “creative” like a human, but it does create new samples by generalizing from what it learned, not just by memorizing. But this also makes me wonder about how human brains work. Isn't human creativity just using patters that the brain learned previously to create something new? So maybe it is similar to human creativity.</paragraph></document>",
          "document": "The way I think about it is that diffusion models aren’t memorizing images at all more like they’re learning the patterns that make something look like the kind of data they were trained on since during training every image gets completely scrambled with noise, and the model’s only job is to learn how to slowly turn that noise back into something realistic. And since it starts from pure noise when generating, there’s no stored picture it could “pull out,” so it has to build a new one by following those learned patterns. That’s why the outputs aren’t just copies or nearest neighbors, they’re new images that fit the style of the dataset without being any specific example from it. So in that sense, it’s not “creative” like a human, but it does create new samples by generalizing from what it learned, not just by memorizing. But this also makes me wonder about how human brains work. Isn't human creativity just using patters that the brain learned previously to create something new? So maybe it is similar to human creativity.",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-06T10:16:53.342726+11:00",
          "updated_at": "2025-12-06T14:27:01.85836+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        },
        {
          "id": 17232802,
          "user_id": 1751537,
          "course_id": 84647,
          "thread_id": 7412341,
          "original_id": null,
          "parent_id": 17222924,
          "editor_id": null,
          "number": 6,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I think diffusion creativity stems from the \"spectral bias\" of neural networks, which prioritize learning smooth, global patterns over the sharp irregularities required for memorization. This limitation forces the model to approximate a continuous energy landscape that bridges the gaps between training points. Therefore, generation becomes a form of \"forced interpolation\", where the model constructs novel samples by following smooth statistical gradients into valid, unvisited configurations rather than reproducing specific images.</paragraph></document>",
          "document": "I think diffusion creativity stems from the \"spectral bias\" of neural networks, which prioritize learning smooth, global patterns over the sharp irregularities required for memorization. This limitation forces the model to approximate a continuous energy landscape that bridges the gaps between training points. Therefore, generation becomes a form of \"forced interpolation\", where the model constructs novel samples by following smooth statistical gradients into valid, unvisited configurations rather than reproducing specific images.",
          "flag_count": 0,
          "vote_count": 3,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-06T19:46:32.561955+11:00",
          "updated_at": "2025-12-07T14:53:24.493096+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        },
        {
          "id": 17239919,
          "user_id": 957592,
          "course_id": 84647,
          "thread_id": 7412341,
          "original_id": null,
          "parent_id": 17222924,
          "editor_id": 957592,
          "number": 7,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I came across a new paper presented at NeurIPS 2025. It shows that the amount of time it takes for a diffusion model to start to memorize grows linearly with the size of the dataset meaning that for large datasets the safe training window is huge, which is why diffusion models often generalize well. It also shows that there is some type of implicit regularization happening in the early stages that delay memorization to later stages. Thought it was interesting.</paragraph><paragraph>https://openreview.net/pdf?id=BSZqpqgqM0</paragraph><paragraph/></document>",
          "document": "I came across a new paper presented at NeurIPS 2025. It shows that the amount of time it takes for a diffusion model to start to memorize grows linearly with the size of the dataset meaning that for large datasets the safe training window is huge, which is why diffusion models often generalize well. It also shows that there is some type of implicit regularization happening in the early stages that delay memorization to later stages. Thought it was interesting.\n\nhttps://openreview.net/pdf?id=BSZqpqgqM0\n\n",
          "flag_count": 0,
          "vote_count": 3,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-07T12:21:19.57371+11:00",
          "updated_at": "2025-12-11T13:45:33.652469+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 17296612,
              "user_id": 622883,
              "course_id": 84647,
              "thread_id": 7412341,
              "original_id": null,
              "parent_id": 17239919,
              "editor_id": null,
              "number": 9,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>Thank you for sharing this! Having skimmed through the paper, there were a couple insights I wanted to share as well. They claim that the time \"r_gen\" when the model begins to generate high-quality samples is constant, regardless of the size of the training set, which was interesting. One key limitation I noticed that they pointed out was that the results they derived were in the SGD case, and that most DMs are trained with Adam; it would be interesting to study this effect in the case of varying optimizers.</paragraph></document>",
              "document": "Thank you for sharing this! Having skimmed through the paper, there were a couple insights I wanted to share as well. They claim that the time \"r_gen\" when the model begins to generate high-quality samples is constant, regardless of the size of the training set, which was interesting. One key limitation I noticed that they pointed out was that the results they derived were in the SGD case, and that most DMs are trained with Adam; it would be interesting to study this effect in the case of varying optimizers.",
              "flag_count": 0,
              "vote_count": 1,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-12-11T13:07:54.657066+11:00",
              "updated_at": "2025-12-11T18:54:32.20037+11:00",
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        },
        {
          "id": 17267692,
          "user_id": 698319,
          "course_id": 84647,
          "thread_id": 7412341,
          "original_id": null,
          "parent_id": 17222924,
          "editor_id": null,
          "number": 8,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>This has been something I've been thinking about, especially given how much diffusion models have taken over generative AI.</paragraph><paragraph>I think the answer lies in how the network learns the score function ∇_x log p_t(x). During training, we're not storing images, we're learning a compressed representation of the gradient field that points toward the data manifold from any noisy point. The network has to generalize this field across the entire space, not just at training points.</paragraph><paragraph>Here's what I find compelling: consider the DDIM trajectories from Figure 6 in the lecture notes. With more training samples, the trajectories become perpendicular to the learned manifold. The network isn't memorizing \"if you see this noise, output this image\", it's learning the geometric structure of the data distribution itself. When we sample from a new random noise vector the model has never seen, it follows this learned vector field to a point on the manifold that may not correspond to any training example.</paragraph><paragraph>That said, I'm not sure we should call this \"creativity\" in any strong sense. The model is fundamentally bounded by the support of the training distribution. It can interpolate smoothly between modes and find novel combinations, but it can't extrapolate to truly new concepts outside what it's seen. The \"creativity\" is more like very sophisticated function approximation over a high-dimensional manifold.</paragraph><paragraph>Maybe a useful analogy: a jazz musician improvising isn't randomly generating notes, they're navigating a learned space of \"what sounds good\" based on training (practice). Diffusion models do something similar, but without any understanding of why certain outputs are good.</paragraph><paragraph>Curious what others think, does the stochastic vs deterministic sampling distinction (DDPM vs DDIM) change how we should think about this?<break/><break/></paragraph></document>",
          "document": "This has been something I've been thinking about, especially given how much diffusion models have taken over generative AI.\n\nI think the answer lies in how the network learns the score function ∇_x log p_t(x). During training, we're not storing images, we're learning a compressed representation of the gradient field that points toward the data manifold from any noisy point. The network has to generalize this field across the entire space, not just at training points.\n\nHere's what I find compelling: consider the DDIM trajectories from Figure 6 in the lecture notes. With more training samples, the trajectories become perpendicular to the learned manifold. The network isn't memorizing \"if you see this noise, output this image\", it's learning the geometric structure of the data distribution itself. When we sample from a new random noise vector the model has never seen, it follows this learned vector field to a point on the manifold that may not correspond to any training example.\n\nThat said, I'm not sure we should call this \"creativity\" in any strong sense. The model is fundamentally bounded by the support of the training distribution. It can interpolate smoothly between modes and find novel combinations, but it can't extrapolate to truly new concepts outside what it's seen. The \"creativity\" is more like very sophisticated function approximation over a high-dimensional manifold.\n\nMaybe a useful analogy: a jazz musician improvising isn't randomly generating notes, they're navigating a learned space of \"what sounds good\" based on training (practice). Diffusion models do something similar, but without any understanding of why certain outputs are good.\n\nCurious what others think, does the stochastic vs deterministic sampling distinction (DDPM vs DDIM) change how we should think about this?\n\n",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-09T11:04:05.767745+11:00",
          "updated_at": "2025-12-12T15:05:10.666326+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    }
  ]
}