{
  "id": 7242190,
  "user_id": 967449,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 212,
  "type": "post",
  "title": "HW9 Q3: Ordinary Softmax Multihead Attention Implementation",
  "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/F8JmNchwPy28TvZzI15BUKYv\" width=\"570\" height=\"1342\"/></figure><paragraph/></document>",
  "document": "",
  "category": "Problem Sets",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 0,
  "view_count": 228,
  "unique_view_count": 89,
  "vote_count": 0,
  "reply_count": 3,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-11-03T03:29:07.493885+11:00",
  "updated_at": "2025-12-16T16:17:58.851111+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16851432,
      "user_id": 606786,
      "course_id": 84647,
      "thread_id": 7242190,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Problem context: </paragraph><paragraph>As you can guess, this is a former exam problem. It is intended to make sure you understand basic multi-head attention at the level of code as well as conceptually. </paragraph><paragraph>The key thing to realize is that the heads are all folded together in some places and pulled apart elsewhere. </paragraph></document>",
      "document": "Problem context: \n\nAs you can guess, this is a former exam problem. It is intended to make sure you understand basic multi-head attention at the level of code as well as conceptually. \n\nThe key thing to realize is that the heads are all folded together in some places and pulled apart elsewhere. ",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-11-03T09:16:58.941609+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16873772,
          "user_id": 1665080,
          "course_id": 84647,
          "thread_id": 7242190,
          "original_id": null,
          "parent_id": 16851432,
          "editor_id": null,
          "number": 2,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>It was mentioned in lecture that in multi-head attention we typically break the input itself into <code>d_model // num_heads</code> or <code>d_k</code> size chunks and process them independently for each attention head. This should have resulted in the creation of <code>num_heads</code> number of <code>(d_k, d_k)</code> shaped matrices for key, query and value weights.  </paragraph><paragraph>In this problem, however, we are processing the entire input (all <code>d_model</code> dimensions) for each attention head. This is resulting in the creation of <code>num_heads</code> number of <code>(d_k, d_model)</code> shaped matrices for key, query and value weights.</paragraph><paragraph>Are both the approaches valid and we just use one based on what is more computationally affordable? Or is one seen more frequently than the other in practice? Thanks! </paragraph></document>",
          "document": "It was mentioned in lecture that in multi-head attention we typically break the input itself into d_model // num_heads or d_k size chunks and process them independently for each attention head. This should have resulted in the creation of num_heads number of (d_k, d_k) shaped matrices for key, query and value weights.  \n\nIn this problem, however, we are processing the entire input (all d_model dimensions) for each attention head. This is resulting in the creation of num_heads number of (d_k, d_model) shaped matrices for key, query and value weights.\n\nAre both the approaches valid and we just use one based on what is more computationally affordable? Or is one seen more frequently than the other in practice? Thanks! ",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-11-04T20:43:43.745149+11:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16876892,
              "user_id": 606786,
              "course_id": 84647,
              "thread_id": 7242190,
              "original_id": null,
              "parent_id": 16873772,
              "editor_id": null,
              "number": 3,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>The approach shown in this problem is more popular and common. These are all design choices to play with computation and parameter counts.</paragraph><paragraph/></document>",
              "document": "The approach shown in this problem is more popular and common. These are all design choices to play with computation and parameter counts.\n\n",
              "flag_count": 0,
              "vote_count": 1,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-11-05T05:16:31.408126+11:00",
              "updated_at": "2025-11-05T05:17:12.773025+11:00",
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        }
      ]
    }
  ]
}