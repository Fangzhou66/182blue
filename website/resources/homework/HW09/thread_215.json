{
  "id": 7242208,
  "user_id": 967449,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 215,
  "type": "post",
  "title": "HW9 Q6: Kernelized Linear Attention (Part 1)",
  "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/lJAtk3sNnUUMGxUXHxP4Leng\" width=\"476\" height=\"1358\"/></figure><paragraph/></document>",
  "document": "",
  "category": "Problem Sets",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 1,
  "view_count": 395,
  "unique_view_count": 138,
  "vote_count": 0,
  "reply_count": 3,
  "unresolved_count": 1,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-11-03T03:32:33.815475+11:00",
  "updated_at": "2025-12-17T06:00:27.436723+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16851344,
      "user_id": 606786,
      "course_id": 84647,
      "thread_id": 7242208,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Problem context: </paragraph><paragraph>This is part 1 of a longer two-part problem that is getting you to engage with both attention, and one key computational complexity bottleneck in the use of attention mechanisms. <break/><break/>This problem (and its second part that we have put on next week's HW just to keep this HW short) is connected in spirit to both state-space models and to the problem you saw on \"ridge attention.\" </paragraph></document>",
      "document": "Problem context: \n\nThis is part 1 of a longer two-part problem that is getting you to engage with both attention, and one key computational complexity bottleneck in the use of attention mechanisms. \n\nThis problem (and its second part that we have put on next week's HW just to keep this HW short) is connected in spirit to both state-space models and to the problem you saw on \"ridge attention.\" ",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-11-03T09:11:31.346174+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 16938980,
      "user_id": 636231,
      "course_id": 84647,
      "thread_id": 7242208,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>For part (b)(ii), is the corresponding feature map for the quadratic kernel the same as in the linked Wikipedia article on polynomial kernels? I am assuming we still need to show the full derivation of it.</paragraph></document>",
      "document": "For part (b)(ii), is the corresponding feature map for the quadratic kernel the same as in the linked Wikipedia article on polynomial kernels? I am assuming we still need to show the full derivation of it.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-11-10T13:12:00.070608+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 17185379,
          "user_id": 1751537,
          "course_id": 84647,
          "thread_id": 7242208,
          "original_id": null,
          "parent_id": 16938980,
          "editor_id": null,
          "number": 3,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I think it's the same since the problem explicitly asks to \"Derive\" the map2. We should demonstrate the algebraic expansion of the kernel function to prove how the specific components of </paragraph><math>\\phi(\\cdot)</math><paragraph>are constructed.</paragraph></document>",
          "document": "I think it's the same since the problem explicitly asks to \"Derive\" the map2. We should demonstrate the algebraic expansion of the kernel function to prove how the specific components of \n\n$$\\phi(\\cdot)$$\n\nare constructed.",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-12-03T16:33:07.057307+11:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    }
  ]
}