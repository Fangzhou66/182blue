{
  "id": 6989287,
  "user_id": 967449,
  "course_id": 84647,
  "original_id": null,
  "editor_id": 606786,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 84,
  "type": "post",
  "title": "HW3 Q2",
  "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/BjfQ713wMpOYRSamgNKdUfDr\" width=\"658\" height=\"99.11297071129707\"/></figure><paragraph/><paragraph>Problem context: This is a critically important homework problem as it has you complement the analytically driven understanding you were given in lecture with seeing what happens in an actual toy neural net. This problem touches on many things that you have seen in the previous few lectures. </paragraph></document>",
  "document": "\n\nProblem context: This is a critically important homework problem as it has you complement the analytically driven understanding you were given in lecture with seeing what happens in an actual toy neural net. This problem touches on many things that you have seen in the previous few lectures. ",
  "category": "Problem Sets",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 0,
  "view_count": 399,
  "unique_view_count": 113,
  "vote_count": 0,
  "reply_count": 13,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-09-20T15:09:59.1106+10:00",
  "updated_at": "2025-12-18T09:55:07.231697+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16289439,
      "user_id": 906474,
      "course_id": 84647,
      "thread_id": 6989287,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Are we expect part d's diagram to be same as part c's like this?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ifmOj176aZlduazJCsxFeXkd\" width=\"655\" height=\"393\"/></figure><paragraph><break/>Because I used <break/><break/>scale = math.sqrt(layer.out_features / layer.in_features)<break/><break/> in part d and the graph (below) is still not flattened<break/><break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/3ZxnbJ0UTQUdF4wgz8pmTAxn\" width=\"655\" height=\"385.0756143667297\"/></figure></document>",
      "document": "Are we expect part d's diagram to be same as part c's like this?\n\n\nBecause I used \n\nscale = math.sqrt(layer.out_features / layer.in_features)\n\n in part d and the graph (below) is still not flattened\n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-22T07:27:46.822076+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16289914,
          "user_id": 906474,
          "course_id": 84647,
          "thread_id": 6989287,
          "original_id": null,
          "parent_id": 16289439,
          "editor_id": null,
          "number": 2,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>My muP loss graph also ends up like this instead of converging to a single learning rate like what is shown from the lecture<break/><break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ScWN0vDGVxwwYgIRgcShlymM\" width=\"624\" height=\"347.84180790960454\"/></figure><paragraph/></document>",
          "document": "My muP loss graph also ends up like this instead of converging to a single learning rate like what is shown from the lecture\n\n\n\n",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-22T08:02:42.816969+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        },
        {
          "id": 16294429,
          "user_id": 967944,
          "course_id": 84647,
          "thread_id": 6989287,
          "original_id": null,
          "parent_id": 16289439,
          "editor_id": null,
          "number": 3,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>The plot for part d should match the plot for part c. Try to give it another shot at what the correct per-layer scaling should be -- if you do the algebra, there is an exact solution.<break/><break/>Bottom: It looks like your muP curves are not training properly. A correct implementation should look like what was shown in lecture, and the loss should be &lt; 0.5 (as Adam achieves). You may have a learning rate for the input/output layers that are too high, see the comment in the code block for part c.</paragraph></document>",
          "document": "The plot for part d should match the plot for part c. Try to give it another shot at what the correct per-layer scaling should be -- if you do the algebra, there is an exact solution.\n\nBottom: It looks like your muP curves are not training properly. A correct implementation should look like what was shown in lecture, and the loss should be < 0.5 (as Adam achieves). You may have a learning rate for the input/output layers that are too high, see the comment in the code block for part c.",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-22T13:48:10.96574+10:00",
          "updated_at": "2025-09-26T07:33:08.825107+10:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16309701,
              "user_id": 906474,
              "course_id": 84647,
              "thread_id": 6989287,
              "original_id": null,
              "parent_id": 16294429,
              "editor_id": null,
              "number": 5,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>I feel like Adam will cancel the scaling effect with momentum, so there is no way to achieve muP during forward pass. Or if that's wrong, could you give some hints?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/I8FApNWVSCJhXfHVv4ZwHxdB\" width=\"590\" height=\"319.7831978319783\"/></figure></document>",
              "document": "I feel like Adam will cancel the scaling effect with momentum, so there is no way to achieve muP during forward pass. Or if that's wrong, could you give some hints?",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-09-23T13:09:39.053061+10:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": [
                {
                  "id": 16349969,
                  "user_id": 967944,
                  "course_id": 84647,
                  "thread_id": 6989287,
                  "original_id": null,
                  "parent_id": 16309701,
                  "editor_id": null,
                  "number": 10,
                  "type": "comment",
                  "kind": "normal",
                  "content": "<document version=\"2.0\"><paragraph>You're right in the intuition that Adam will result in the same update to the parameters, regardless of the magnitude of the gradient. However, can you think of a way that the <italic>same</italic> update to parameters can result in a <italic>different</italic> update to the <italic>features</italic>?</paragraph><paragraph/></document>",
                  "document": "You're right in the intuition that Adam will result in the same update to the parameters, regardless of the magnitude of the gradient. However, can you think of a way that the same update to parameters can result in a different update to the features?\n\n",
                  "flag_count": 0,
                  "vote_count": 1,
                  "is_endorsed": false,
                  "is_anonymous": false,
                  "is_private": false,
                  "is_resolved": false,
                  "created_by_bot_id": null,
                  "created_at": "2025-09-26T06:50:40.353085+10:00",
                  "updated_at": "2025-09-26T07:33:11.697686+10:00",
                  "deleted_at": null,
                  "anonymous_id": 0,
                  "vote": 0,
                  "comments": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": 16335870,
      "user_id": 954911,
      "course_id": 84647,
      "thread_id": 6989287,
      "original_id": null,
      "parent_id": null,
      "editor_id": 954911,
      "number": 7,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/bmjzedat0cOCeXfTuzIQ4bvp\" width=\"655\" height=\"371.4657534246575\"/></figure><paragraph>Im getting something like this for my mu p graphs. It seems like most of the layer widths have similar optimal lrs but for the big ones they have different ones? Intuitively I'm thinking (if my graph isn't wrong) that this is because the widths aren't large enough where in the limit the difference should be the same?</paragraph><paragraph>Also for d I checked my algebra and it seems right but it seems like the rms norm going to is 0? (the graph is in scale of 1e-5) For context I also fudged the input layer like is noted in c for similar results. It seems like this could be though because the hyperaparmeters used are different in c and d during the training step.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fzZBQQWaSKqkaDkZM8h5s1dM\" width=\"628\" height=\"378.09104258443466\"/></figure><paragraph/></document>",
      "document": "Im getting something like this for my mu p graphs. It seems like most of the layer widths have similar optimal lrs but for the big ones they have different ones? Intuitively I'm thinking (if my graph isn't wrong) that this is because the widths aren't large enough where in the limit the difference should be the same?\n\nAlso for d I checked my algebra and it seems right but it seems like the rms norm going to is 0? (the graph is in scale of 1e-5) For context I also fudged the input layer like is noted in c for similar results. It seems like this could be though because the hyperaparmeters used are different in c and d during the training step.\n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-25T08:08:36.125484+10:00",
      "updated_at": "2025-09-25T08:15:54.324512+10:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16350154,
          "user_id": 967944,
          "course_id": 84647,
          "thread_id": 6989287,
          "original_id": null,
          "parent_id": 16335870,
          "editor_id": null,
          "number": 13,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Your muP graphs look reasonable. This is what you get if you implement the naive muP update. In the notebook, there are a set of hints -- if you tune the 'fudge factors' for the input/output layers, you should be able to get the optimal LRs to align more:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/16RKNe7gXeAJ5xa5vEHrVA7C\" width=\"609\" height=\"345.0689127105666\"/></figure><paragraph>Your graph for d) looks incorrect. You should be able to match the graph you created in part c. One possible hint: your per-weight multipliers should be &gt; 1. </paragraph><paragraph/></document>",
          "document": "Your muP graphs look reasonable. This is what you get if you implement the naive muP update. In the notebook, there are a set of hints -- if you tune the 'fudge factors' for the input/output layers, you should be able to get the optimal LRs to align more:\n\nYour graph for d) looks incorrect. You should be able to match the graph you created in part c. One possible hint: your per-weight multipliers should be > 1. \n\n",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-26T07:01:11.037638+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16357252,
              "user_id": 954911,
              "course_id": 84647,
              "thread_id": 6989287,
              "original_id": null,
              "parent_id": 16350154,
              "editor_id": null,
              "number": 15,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>hmm interesting I actually did include the fudge stuff but I still get those results. Curious what's \"naive\" about this muP approach. Is it that the sizes of the models are too small?</paragraph></document>",
              "document": "hmm interesting I actually did include the fudge stuff but I still get those results. Curious what's \"naive\" about this muP approach. Is it that the sizes of the models are too small?",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-09-26T16:59:35.323962+10:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        }
      ]
    },
    {
      "id": 16336721,
      "user_id": 647625,
      "course_id": 84647,
      "thread_id": 6989287,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 8,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>For implementing the hint for part e in AdamMuP, is there an easy way to tell if a parameter is for the output layer or input layer?</paragraph></document>",
      "document": "For implementing the hint for part e in AdamMuP, is there an easy way to tell if a parameter is for the output layer or input layer?",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-25T08:59:42.742798+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16349994,
          "user_id": 967944,
          "course_id": 84647,
          "thread_id": 6989287,
          "original_id": null,
          "parent_id": 16336721,
          "editor_id": null,
          "number": 12,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Yes, you can check the shapes of the matrices. The input layer should always have an input dimension of 768, and the output will have an output dimension of 10. </paragraph></document>",
          "document": "Yes, you can check the shapes of the matrices. The input layer should always have an input dimension of 768, and the output will have an output dimension of 10. ",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-26T06:52:23.231258+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16354371,
              "user_id": 647625,
              "course_id": 84647,
              "thread_id": 6989287,
              "original_id": null,
              "parent_id": 16349994,
              "editor_id": null,
              "number": 14,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>Thanks! I realized that the inner loop also iterates through parameters in order, so you can also use enumerate in the for loop and check the value of the index.</paragraph></document>",
              "document": "Thanks! I realized that the inner loop also iterates through parameters in order, so you can also use enumerate in the for loop and check the value of the index.",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-09-26T11:41:26.355891+10:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        }
      ]
    },
    {
      "id": 16348130,
      "user_id": 679296,
      "course_id": 84647,
      "thread_id": 6989287,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 9,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>I think there are 2 part es in the notebook.</paragraph></document>",
      "document": "I think there are 2 part es in the notebook.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-26T05:00:47.384036+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16349988,
          "user_id": 967944,
          "course_id": 84647,
          "thread_id": 6989287,
          "original_id": null,
          "parent_id": 16348130,
          "editor_id": null,
          "number": 11,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Thank you, you are right, the Shampoo portion should be part f.</paragraph></document>",
          "document": "Thank you, you are right, the Shampoo portion should be part f.",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-26T06:51:54.727403+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    }
  ]
}