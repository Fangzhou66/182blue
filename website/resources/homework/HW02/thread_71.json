{
  "id": 6954665,
  "user_id": 967944,
  "course_id": 84647,
  "original_id": null,
  "editor_id": 606786,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 71,
  "type": "post",
  "title": "HW2 Q4",
  "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/2jfaBZof4468uTUnWThv68zZ\" width=\"658\" height=\"748.8177083333334\"/></figure><paragraph>Problem Context:</paragraph><paragraph>This engages with new material (the Taylor expansion perspective taught in lecture) and builds on the visualizations you saw in the first two discussion sections as well as the analytic part of the previous discussion. This problem requires you to translate what is being asked into math yourself, and then use those concepts to visualize what is going on by modifying a given jupyter notebook.  </paragraph><paragraph>This is a Deep Learning course, not a math course. Of course, we leverage math. This problem has lots of conceptual moving parts, and by trying to understand what is being asked and how to answer it, you will hopefully straighten out those concepts in your mind.</paragraph><paragraph>Along the way, this will also likely help many of you strengthen your understanding of classic dimensionality-reduction via PCA in machine learning. In particular, what are the dimensionality-reduced features when you apply them to new data...</paragraph></document>",
  "document": "Problem Context:\n\nThis engages with new material (the Taylor expansion perspective taught in lecture) and builds on the visualizations you saw in the first two discussion sections as well as the analytic part of the previous discussion. This problem requires you to translate what is being asked into math yourself, and then use those concepts to visualize what is going on by modifying a given jupyter notebook.  \n\nThis is a Deep Learning course, not a math course. Of course, we leverage math. This problem has lots of conceptual moving parts, and by trying to understand what is being asked and how to answer it, you will hopefully straighten out those concepts in your mind.\n\nAlong the way, this will also likely help many of you strengthen your understanding of classic dimensionality-reduction via PCA in machine learning. In particular, what are the dimensionality-reduced features when you apply them to new data...",
  "category": "Problem Sets",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 0,
  "view_count": 431,
  "unique_view_count": 141,
  "vote_count": 0,
  "reply_count": 4,
  "unresolved_count": 1,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-09-13T13:16:32.338449+10:00",
  "updated_at": "2025-12-17T17:48:11.709797+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16271368,
      "user_id": 1751442,
      "course_id": 84647,
      "thread_id": 6954665,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>I am wondering when plotting the feature matrix, why are we putting all the parameters together to study the gradients (as in the demonstrated <code>compute_svd_plot_features</code> function) instead of investigating each layer separately (like save the gradients for each layer separately and calculate them separately) ? I plot out both the svd matrices and seems like the number of the large singular values are always around 2 or 3 no matter how I plot it. I am not sure whether I am seeing the expected results. Do others encounter the same issues? </paragraph><paragraph/></document>",
      "document": "I am wondering when plotting the feature matrix, why are we putting all the parameters together to study the gradients (as in the demonstrated compute_svd_plot_features function) instead of investigating each layer separately (like save the gradients for each layer separately and calculate them separately) ? I plot out both the svd matrices and seems like the number of the large singular values are always around 2 or 3 no matter how I plot it. I am not sure whether I am seeing the expected results. Do others encounter the same issues? \n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": false,
      "created_by_bot_id": null,
      "created_at": "2025-09-20T06:44:10.413533+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 16292293,
      "user_id": 647625,
      "course_id": 84647,
      "thread_id": 6954665,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>For part b, what exactly are the feature matrix and principle features? I'm really confused by what their definitions are from the problem statement.</paragraph></document>",
      "document": "For part b, what exactly are the feature matrix and principle features? I'm really confused by what their definitions are from the problem statement.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-09-22T11:02:41.794592+10:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16295042,
          "user_id": 954911,
          "course_id": 84647,
          "thread_id": 6954665,
          "original_id": null,
          "parent_id": 16292293,
          "editor_id": null,
          "number": 3,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>I would suggest trying to complete the code and read the skeleton code to understand the problem in more detail. This helped me </paragraph><paragraph>To answer your question - our network takes in a scalar x as input. If you see how the feature matrix is constructed, you'll find that the entries in each row corresponds to the gradient of the minibatch loss with one point (x_i for row i). The principle features mean the coordinates when you project each point (the gradients in each row of the feature matrix) into it's principal component space. The context is that like prof showed in a prev lecture, we want to see how the gradient is composed of different vectors and how it moves at different speeds in different directions.</paragraph><paragraph>Hope that helps!</paragraph></document>",
          "document": "I would suggest trying to complete the code and read the skeleton code to understand the problem in more detail. This helped me \n\nTo answer your question - our network takes in a scalar x as input. If you see how the feature matrix is constructed, you'll find that the entries in each row corresponds to the gradient of the minibatch loss with one point (x_i for row i). The principle features mean the coordinates when you project each point (the gradients in each row of the feature matrix) into it's principal component space. The context is that like prof showed in a prev lecture, we want to see how the gradient is composed of different vectors and how it moves at different speeds in different directions.\n\nHope that helps!",
          "flag_count": 0,
          "vote_count": 0,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-09-22T14:54:32.819268+10:00",
          "updated_at": null,
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": [
            {
              "id": 16304878,
              "user_id": 647625,
              "course_id": 84647,
              "thread_id": 6954665,
              "original_id": null,
              "parent_id": 16295042,
              "editor_id": null,
              "number": 4,
              "type": "comment",
              "kind": "normal",
              "content": "<document version=\"2.0\"><paragraph>That makes sense, thanks!</paragraph></document>",
              "document": "That makes sense, thanks!",
              "flag_count": 0,
              "vote_count": 0,
              "is_endorsed": false,
              "is_anonymous": false,
              "is_private": false,
              "is_resolved": false,
              "created_by_bot_id": null,
              "created_at": "2025-09-23T07:51:23.673375+10:00",
              "updated_at": null,
              "deleted_at": null,
              "anonymous_id": 0,
              "vote": 0,
              "comments": []
            }
          ]
        }
      ]
    }
  ]
}