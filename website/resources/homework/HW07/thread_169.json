{
  "id": 7166396,
  "user_id": 591161,
  "course_id": 84647,
  "original_id": null,
  "editor_id": null,
  "accepted_id": null,
  "duplicate_id": null,
  "number": 169,
  "type": "post",
  "title": "HW7 Q3: Auto-encoder: Learning without Labels",
  "content": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/c5EdUpX5ccLSEAbJ4aqmjitc\" width=\"581\" height=\"83.42564102564103\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/ytx501ettcfSxxxEgmSW3YCF\" width=\"581\" height=\"781.1308411214953\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/5U0l0jqjbDQQtmQCQN3uOIWL\" width=\"586\" height=\"92\"/></figure><paragraph/></document>",
  "document": "",
  "category": "Problem Sets",
  "subcategory": "",
  "subsubcategory": "",
  "flag_count": 0,
  "star_count": 0,
  "view_count": 338,
  "unique_view_count": 117,
  "vote_count": 0,
  "reply_count": 3,
  "unresolved_count": 0,
  "is_locked": false,
  "is_pinned": false,
  "is_private": false,
  "is_endorsed": false,
  "is_answered": false,
  "is_student_answered": false,
  "is_staff_answered": false,
  "is_archived": false,
  "is_anonymous": false,
  "is_megathread": true,
  "anonymous_comments": false,
  "approved_status": "approved",
  "created_at": "2025-10-21T05:49:13.820227+11:00",
  "updated_at": "2025-12-17T17:47:11.186137+11:00",
  "deleted_at": null,
  "pinned_at": null,
  "anonymous_id": 0,
  "vote": 0,
  "is_seen": false,
  "is_starred": false,
  "is_watched": null,
  "glanced_at": null,
  "new_reply_count": 0,
  "duplicate_title": null,
  "answers": [],
  "comments": [
    {
      "id": 16679649,
      "user_id": 591161,
      "course_id": 84647,
      "thread_id": 7166396,
      "original_id": null,
      "parent_id": null,
      "editor_id": null,
      "number": 1,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>Problem context: </paragraph><paragraph>Autoencoders are conceptually important for understanding Deep Learning, especially since they form the foundation for understanding self-supervision and hence all modern pretraining.</paragraph></document>",
      "document": "Problem context: \n\nAutoencoders are conceptually important for understanding Deep Learning, especially since they form the foundation for understanding self-supervision and hence all modern pretraining.",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-10-21T05:55:40.786978+11:00",
      "updated_at": null,
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": []
    },
    {
      "id": 16691674,
      "user_id": 1294638,
      "course_id": 84647,
      "thread_id": 7166396,
      "original_id": null,
      "parent_id": null,
      "editor_id": 1294638,
      "number": 2,
      "type": "comment",
      "kind": "normal",
      "content": "<document version=\"2.0\"><paragraph>I have a question for MAE: <break/><break/>the problem states \"During training, the mean squared error between the unmasked part of $x$ and the corresponding part of $x'$ is minimized.\"<break/><break/>my understanding was that MAE is trained by predicting the masked values, not the unmasked values. That is, given input ( <italic>x</italic> ) and binary mask (<italic>mask)</italic>, we calculate loss as:<break/><break/>x_hat = decode(encode(x * mask))<break/>loss(x_hat * (1-mask), x * (1-mask))<break/><break/>Such that we are calculating the reconstruction accuracy of the missing (masked) features, not the features which were left unmasked?<break/><break/>Or am I mistaken on what the term \"unmasked\" means?</paragraph><paragraph/></document>",
      "document": "I have a question for MAE: \n\nthe problem states \"During training, the mean squared error between the unmasked part of $x$ and the corresponding part of $x'$ is minimized.\"\n\nmy understanding was that MAE is trained by predicting the masked values, not the unmasked values. That is, given input ( x ) and binary mask (mask), we calculate loss as:\n\nx_hat = decode(encode(x * mask))\nloss(x_hat * (1-mask), x * (1-mask))\n\nSuch that we are calculating the reconstruction accuracy of the missing (masked) features, not the features which were left unmasked?\n\nOr am I mistaken on what the term \"unmasked\" means?\n\n",
      "flag_count": 0,
      "vote_count": 0,
      "is_endorsed": false,
      "is_anonymous": false,
      "is_private": false,
      "is_resolved": true,
      "created_by_bot_id": null,
      "created_at": "2025-10-21T15:01:32.29447+11:00",
      "updated_at": "2025-10-21T15:02:51.020622+11:00",
      "deleted_at": null,
      "anonymous_id": 0,
      "vote": 0,
      "comments": [
        {
          "id": 16719511,
          "user_id": 967944,
          "course_id": 84647,
          "thread_id": 7166396,
          "original_id": null,
          "parent_id": 16691674,
          "editor_id": 967944,
          "number": 3,
          "type": "comment",
          "kind": "normal",
          "content": "<document version=\"2.0\"><paragraph>Yes, you are right, I believe that is an error in the problem description. The MAE works by defining a reconstruction loss on the <italic>masked</italic> features (as the unmasked features are trivially predictable)</paragraph></document>",
          "document": "Yes, you are right, I believe that is an error in the problem description. The MAE works by defining a reconstruction loss on the masked features (as the unmasked features are trivially predictable)",
          "flag_count": 0,
          "vote_count": 1,
          "is_endorsed": false,
          "is_anonymous": false,
          "is_private": false,
          "is_resolved": false,
          "created_by_bot_id": null,
          "created_at": "2025-10-23T10:11:47.976302+11:00",
          "updated_at": "2025-12-09T15:26:39.627258+11:00",
          "deleted_at": null,
          "anonymous_id": 0,
          "vote": 0,
          "comments": []
        }
      ]
    }
  ]
}